{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-31T16:32:25.990352Z",
     "iopub.status.busy": "2024-10-31T16:32:25.990059Z",
     "iopub.status.idle": "2024-10-31T16:32:27.044873Z",
     "shell.execute_reply": "2024-10-31T16:32:27.043929Z",
     "shell.execute_reply.started": "2024-10-31T16:32:25.990318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wiki-articles-2/cleaned_file.csv\n",
      "/kaggle/input/tokenizer/tokenizer.json\n",
      "/kaggle/input/tokenizer/tokenizer_config.json\n",
      "/kaggle/input/tokenizer/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:32:54.969315Z",
     "iopub.status.busy": "2024-10-31T16:32:54.968367Z",
     "iopub.status.idle": "2024-10-31T16:32:54.973893Z",
     "shell.execute_reply": "2024-10-31T16:32:54.972952Z",
     "shell.execute_reply.started": "2024-10-31T16:32:54.969274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# At the top of your script\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*os.fork.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message='.*autocast.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:32:57.677678Z",
     "iopub.status.busy": "2024-10-31T16:32:57.677061Z",
     "iopub.status.idle": "2024-10-31T16:33:01.006123Z",
     "shell.execute_reply": "2024-10-31T16:33:01.005043Z",
     "shell.execute_reply.started": "2024-10-31T16:32:57.677617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:01.008402Z",
     "iopub.status.busy": "2024-10-31T16:33:01.007899Z",
     "iopub.status.idle": "2024-10-31T16:33:13.842075Z",
     "shell.execute_reply": "2024-10-31T16:33:13.841038Z",
     "shell.execute_reply.started": "2024-10-31T16:33:01.008363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.10.1 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:13.844458Z",
     "iopub.status.busy": "2024-10-31T16:33:13.844120Z",
     "iopub.status.idle": "2024-10-31T16:33:13.857424Z",
     "shell.execute_reply": "2024-10-31T16:33:13.856389Z",
     "shell.execute_reply.started": "2024-10-31T16:33:13.844424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x799082842e30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:13.859242Z",
     "iopub.status.busy": "2024-10-31T16:33:13.858614Z",
     "iopub.status.idle": "2024-10-31T16:33:13.863604Z",
     "shell.execute_reply": "2024-10-31T16:33:13.862723Z",
     "shell.execute_reply.started": "2024-10-31T16:33:13.859199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imprtant Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:13.865664Z",
     "iopub.status.busy": "2024-10-31T16:33:13.865377Z",
     "iopub.status.idle": "2024-10-31T16:33:20.269332Z",
     "shell.execute_reply": "2024-10-31T16:33:20.268503Z",
     "shell.execute_reply.started": "2024-10-31T16:33:13.865634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "# from .metrics import TranslationMetrics\n",
    "# from .lr_finder import LRFinder\n",
    "# from .distributed import DistributedManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:20.272045Z",
     "iopub.status.busy": "2024-10-31T16:33:20.271097Z",
     "iopub.status.idle": "2024-10-31T16:33:21.040906Z",
     "shell.execute_reply": "2024-10-31T16:33:21.039907Z",
     "shell.execute_reply.started": "2024-10-31T16:33:20.271947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer from the saved directory\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/input/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:33:22.726805Z",
     "iopub.status.busy": "2024-10-31T16:33:22.726084Z",
     "iopub.status.idle": "2024-10-31T16:33:22.732843Z",
     "shell.execute_reply": "2024-10-31T16:33:22.731292Z",
     "shell.execute_reply.started": "2024-10-31T16:33:22.726765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define your parameters\n",
    "model_params = {\n",
    "    'vocab_size': 30000,\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'd_ff': 1024,\n",
    "    'max_seq_length': 256,\n",
    "    'dropout': 0.1,\n",
    "    'log_attention_weights': False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Basic configuration\n",
    "data_dict = {\n",
    "    'file_path': \"/kaggle/input/wiki-articles-2/cleaned_file.csv\",\n",
    "    'batch_size': 16,\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'num_workers': 2,  # Set to 0 for debugging\n",
    "    'pin_memory': False  # No need for CPU training\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = FlexibleDataPipeline(config=data_dict, tokenizer=tokenizer)\n",
    "train_loader, val_loader, test_loader = pipeline.create_dataloaders()\n",
    "\n",
    "\n",
    "\n",
    "# For Single GPU\n",
    "data_dict = {\n",
    "    'file_path': \"/kaggle/input/wiki-articles-2/cleaned_file.csv\",\n",
    "    'batch_size': 16,\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "pipeline = FlexibleDataPipeline(config=data_dict, tokenizer=tokenizer)\n",
    "train_loader, val_loader, test_loader = pipeline.create_dataloaders()\n",
    "\n",
    "\n",
    "# For Multi GPU\n",
    "import os\n",
    "os.environ['WORLD_SIZE'] = '2'  # Number of GPUs\n",
    "os.environ['RANK'] = '0'  # Local rank of this process\n",
    "os.environ['LOCAL_RANK'] = '0'  # Local rank of this process\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '29500'\n",
    "\n",
    "# Setup distributed training\n",
    "is_distributed = setup_distributed()\n",
    "\n",
    "data_dict = {\n",
    "    'file_path': \"/kaggle/input/wiki-articles-2/cleaned_file.csv\",\n",
    "    'batch_size': 32,  # Will be divided by number of GPUs\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1,\n",
    "    'max_seq_length': 256,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "pipeline = FlexibleDataPipeline(config=data_dict, tokenizer=tokenizer)\n",
    "train_loader, val_loader, test_loader = pipeline.create_dataloaders(distributed=is_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:01.616628Z",
     "iopub.status.busy": "2024-10-31T16:34:01.616217Z",
     "iopub.status.idle": "2024-10-31T16:34:03.535693Z",
     "shell.execute_reply": "2024-10-31T16:34:03.534894Z",
     "shell.execute_reply.started": "2024-10-31T16:34:01.616589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/kaggle/input/wiki-articles-2/cleaned_file.csv\")\n",
    "subset = dataset.head(16)\n",
    "subset.to_csv(\"./subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:06.501209Z",
     "iopub.status.busy": "2024-10-31T16:34:06.500843Z",
     "iopub.status.idle": "2024-10-31T16:34:06.526939Z",
     "shell.execute_reply": "2024-10-31T16:34:06.525913Z",
     "shell.execute_reply.started": "2024-10-31T16:34:06.501176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_length=256):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_sentences[idx]\n",
    "        tgt = self.tgt_sentences[idx]\n",
    "        \n",
    "        src_tokenized = self.tokenizer(\n",
    "            src, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )['input_ids'].squeeze(0)\n",
    "        \n",
    "        tgt_tokenized = self.tokenizer(\n",
    "            tgt, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )['input_ids'].squeeze(0)\n",
    "        \n",
    "        return {'src': src_tokenized, 'tgt': tgt_tokenized}\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        src_batch = torch.stack([item['src'] for item in batch])\n",
    "        tgt_batch = torch.stack([item['tgt'] for item in batch])\n",
    "        return {'src': src_batch, 'tgt': tgt_batch}\n",
    "\n",
    "class FlexibleDataPipeline:\n",
    "    def __init__(self, file_path: str, tokenizer, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the data pipeline with automatic environment detection\n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            tokenizer: Tokenizer instance for processing text\n",
    "            **kwargs: Optional configuration overrides\n",
    "        \"\"\"\n",
    "        # Detect computing environment\n",
    "        self.device_info = self._detect_environment()\n",
    "        \n",
    "        # Set default configuration based on environment\n",
    "        self.config = self._get_default_config()\n",
    "        \n",
    "        # Override defaults with any provided kwargs\n",
    "        self.config.update(kwargs)\n",
    "        \n",
    "        # Store essential components\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load data\n",
    "        self.src_sentences, self.tgt_sentences = self._load_sentences()\n",
    "        \n",
    "        # Print configuration for transparency\n",
    "        self._print_config()\n",
    "\n",
    "    def _detect_environment(self) -> Dict:\n",
    "        \"\"\"Detect the computing environment and return relevant information\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            gpu_names = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
    "            return {\n",
    "                'device': 'cuda',\n",
    "                'num_gpus': num_gpus,\n",
    "                'gpu_names': gpu_names\n",
    "            }\n",
    "        return {\n",
    "            'device': 'cpu',\n",
    "            'num_gpus': 0,\n",
    "            'gpu_names': []\n",
    "        }\n",
    "\n",
    "    def _get_default_config(self) -> Dict:\n",
    "        \"\"\"Get default configuration based on detected environment\"\"\"\n",
    "        is_cpu = self.device_info['device'] == 'cpu'\n",
    "        num_gpus = self.device_info['num_gpus']\n",
    "        \n",
    "        config = {\n",
    "            'batch_size': 8 if is_cpu else (16 * num_gpus if num_gpus > 0 else 16),\n",
    "            'test_size': 0.1,\n",
    "            'val_size': 0.1,\n",
    "            'max_seq_length': 256,\n",
    "            'num_workers': 0,   # No workers for CPU training\n",
    "            'pin_memory': not is_cpu,  # Only use pin_memory for GPU\n",
    "            'device': self.device_info['device']\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def _print_config(self):\n",
    "        \"\"\"Print the current configuration and environment details\"\"\"\n",
    "        print(\"\\n=== Environment Detection ===\")\n",
    "        print(f\"Device: {self.device_info['device'].upper()}\")\n",
    "        if self.device_info['num_gpus'] > 0:\n",
    "            print(f\"Number of GPUs: {self.device_info['num_gpus']}\")\n",
    "            for i, gpu in enumerate(self.device_info['gpu_names']):\n",
    "                print(f\"GPU {i}: {gpu}\")\n",
    "        \n",
    "        print(\"\\n=== Pipeline Configuration ===\")\n",
    "        print(f\"Batch Size: {self.config['batch_size']}\")\n",
    "        print(f\"Number of Workers: {self.config['num_workers']}\")\n",
    "        print(f\"Pin Memory: {self.config['pin_memory']}\")\n",
    "        print(f\"Max Sequence Length: {self.config['max_seq_length']}\")\n",
    "        print(\"===========================\\n\")\n",
    "\n",
    "    def _load_sentences(self) -> Tuple[list, list]:\n",
    "        \"\"\"Load source and target sentences from CSV file\"\"\"\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        return df.iloc[:, 0].tolist(), df.iloc[:, 1].tolist()\n",
    "\n",
    "    def create_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create data loaders optimized for the current environment\n",
    "        Returns:\n",
    "            Tuple of (train_loader, val_loader, test_loader)\n",
    "        \"\"\"\n",
    "        # Split data\n",
    "        src_train, src_temp, tgt_train, tgt_temp = train_test_split(\n",
    "            self.src_sentences, self.tgt_sentences, \n",
    "            test_size=self.config['test_size'] + self.config['val_size'], \n",
    "            random_state=42\n",
    "        )\n",
    "        src_val, src_test, tgt_val, tgt_test = train_test_split(\n",
    "            src_temp, tgt_temp, \n",
    "            test_size=self.config['test_size'] / (self.config['test_size'] + self.config['val_size']), \n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = TranslationDataset(\n",
    "            src_train, tgt_train, self.tokenizer, self.config['max_seq_length']\n",
    "        )\n",
    "        val_dataset = TranslationDataset(\n",
    "            src_val, tgt_val, self.tokenizer, self.config['max_seq_length']\n",
    "        )\n",
    "        test_dataset = TranslationDataset(\n",
    "            src_test, tgt_test, self.tokenizer, self.config['max_seq_length']\n",
    "        )\n",
    "\n",
    "        # Create dataloaders with environment-optimized settings\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=self.config['num_workers'],\n",
    "            pin_memory=self.config['pin_memory'],\n",
    "            collate_fn=TranslationDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        # Use larger batch size for evaluation when possible\n",
    "        eval_batch_size = self.config['batch_size'] * 2\n",
    "        val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=eval_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config['num_workers'],\n",
    "            pin_memory=self.config['pin_memory'],\n",
    "            collate_fn=TranslationDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=eval_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config['num_workers'],\n",
    "            pin_memory=self.config['pin_memory'],\n",
    "            collate_fn=TranslationDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:08.526115Z",
     "iopub.status.busy": "2024-10-31T16:34:08.525166Z",
     "iopub.status.idle": "2024-10-31T16:34:08.539042Z",
     "shell.execute_reply": "2024-10-31T16:34:08.538032Z",
     "shell.execute_reply.started": "2024-10-31T16:34:08.526071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Environment Detection ===\n",
      "Device: CUDA\n",
      "Number of GPUs: 1\n",
      "GPU 0: Tesla P100-PCIE-16GB\n",
      "\n",
      "=== Pipeline Configuration ===\n",
      "Batch Size: 16\n",
      "Number of Workers: 0\n",
      "Pin Memory: True\n",
      "Max Sequence Length: 256\n",
      "===========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with minimal configuration\n",
    "pipeline = FlexibleDataPipeline(\n",
    "#     file_path=\"/kaggle/input/wiki-articles-2/cleaned_file.csv\",\n",
    "    file_path = \"/kaggle/working/subset.csv\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Get dataloaders\n",
    "train_loader, val_loader, test_loader = pipeline.create_dataloaders()\n",
    "\n",
    "# Prepare model for the detected environment\n",
    "# model = YourModel()\n",
    "# model = pipeline.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:11.509915Z",
     "iopub.status.busy": "2024-10-31T16:34:11.509536Z",
     "iopub.status.idle": "2024-10-31T16:34:11.712602Z",
     "shell.execute_reply": "2024-10-31T16:34:11.711812Z",
     "shell.execute_reply.started": "2024-10-31T16:34:11.509880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    src_data = batch['src']\n",
    "    tgt_data = batch['tgt']\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:13.366865Z",
     "iopub.status.busy": "2024-10-31T16:34:13.366008Z",
     "iopub.status.idle": "2024-10-31T16:34:13.375725Z",
     "shell.execute_reply": "2024-10-31T16:34:13.374747Z",
     "shell.execute_reply.started": "2024-10-31T16:34:13.366827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brienne Sidonie Dessaulniers، جو پیشہ ورانہ طور پر Brie Larson کے نام سے مشہور ہیں، ایک امریکی اداکارہ اور فلم ساز ہیں۔\n",
      "Brienne Sidonie Dessaulniers, known professionally as Brie Larson, is an American actress and filmmaker.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(src_data[0], skip_special_tokens=True))\n",
    "print(tokenizer.decode(tgt_data[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model related contstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Let's create some constants to make the code a bit cleaner\n",
    "\n",
    "# Architecture related constants taken from the paper\n",
    "BASELINE_MODEL_NUMBER_OF_LAYERS = 6\n",
    "BASELINE_MODEL_DIMENSION = 512\n",
    "BASELINE_MODEL_NUMBER_OF_HEADS = 8\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
    "\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints') # semi-trained models during training will be dumped here\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries') # location where trained models are located\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data') # training data will be stored here\n",
    "\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
    "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# Special token symbols used later in the data section\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:19.778134Z",
     "iopub.status.busy": "2024-10-31T16:34:19.777725Z",
     "iopub.status.idle": "2024-10-31T16:34:19.791348Z",
     "shell.execute_reply": "2024-10-31T16:34:19.790493Z",
     "shell.execute_reply.started": "2024-10-31T16:34:19.778097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, log_attention_weights=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.log_attention_weights = log_attention_weights\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        if self.log_attention_weights:\n",
    "            return output, attn_probs\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        if self.log_attention_weights:\n",
    "            attn_output, attn_probs = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "            output = self.W_o(self.combine_heads(attn_output))\n",
    "            return output, attn_probs\n",
    "        else:\n",
    "            attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "            output = self.W_o(self.combine_heads(attn_output))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:21.382591Z",
     "iopub.status.busy": "2024-10-31T16:34:21.381926Z",
     "iopub.status.idle": "2024-10-31T16:34:21.388429Z",
     "shell.execute_reply": "2024-10-31T16:34:21.387545Z",
     "shell.execute_reply.started": "2024-10-31T16:34:21.382551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:22.825109Z",
     "iopub.status.busy": "2024-10-31T16:34:22.824032Z",
     "iopub.status.idle": "2024-10-31T16:34:22.835353Z",
     "shell.execute_reply": "2024-10-31T16:34:22.834159Z",
     "shell.execute_reply.started": "2024-10-31T16:34:22.825017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:24.129355Z",
     "iopub.status.busy": "2024-10-31T16:34:24.128903Z",
     "iopub.status.idle": "2024-10-31T16:34:24.139009Z",
     "shell.execute_reply": "2024-10-31T16:34:24.138035Z",
     "shell.execute_reply.started": "2024-10-31T16:34:24.129315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, log_attention_weights=False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.log_attention_weights = log_attention_weights\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, log_attention_weights)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        if self.log_attention_weights:\n",
    "            attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
    "            x = self.norm1(x + self.dropout(attn_output))\n",
    "            ff_output = self.feed_forward(x)\n",
    "            x = self.norm2(x + self.dropout(ff_output))\n",
    "            return x, attn_weights\n",
    "        else:\n",
    "            attn_output = self.self_attn(x, x, x, mask)\n",
    "            x = self.norm1(x + self.dropout(attn_output))\n",
    "            ff_output = self.feed_forward(x)\n",
    "            x = self.norm2(x + self.dropout(ff_output))\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:25.609762Z",
     "iopub.status.busy": "2024-10-31T16:34:25.609381Z",
     "iopub.status.idle": "2024-10-31T16:34:25.620435Z",
     "shell.execute_reply": "2024-10-31T16:34:25.619594Z",
     "shell.execute_reply.started": "2024-10-31T16:34:25.609727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, log_attention_weights):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.log_attention_weights = log_attention_weights\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, log_attention_weights)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, log_attention_weights)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        if self.log_attention_weights:\n",
    "            attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)\n",
    "            x = self.norm1(x + self.dropout(attn_output))\n",
    "            attn_output, cross_attn_weights = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "            x = self.norm2(x + self.dropout(attn_output))\n",
    "            ff_output = self.feed_forward(x)\n",
    "            x = self.norm3(x + self.dropout(ff_output))\n",
    "            return x, self_attn_weights, cross_attn_weights\n",
    "        else:\n",
    "            attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "            x = self.norm1(x + self.dropout(attn_output))\n",
    "            attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "            x = self.norm2(x + self.dropout(attn_output))\n",
    "            ff_output = self.feed_forward(x)\n",
    "            x = self.norm3(x + self.dropout(ff_output))\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:27.320056Z",
     "iopub.status.busy": "2024-10-31T16:34:27.319677Z",
     "iopub.status.idle": "2024-10-31T16:34:27.326138Z",
     "shell.execute_reply": "2024-10-31T16:34:27.325158Z",
     "shell.execute_reply.started": "2024-10-31T16:34:27.320021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_mask(src, tgt):\n",
    "    device = src.device\n",
    "    src_mask = (src != 1).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != 1).unsqueeze(1).unsqueeze(3)\n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:29.025719Z",
     "iopub.status.busy": "2024-10-31T16:34:29.025004Z",
     "iopub.status.idle": "2024-10-31T16:34:29.042878Z",
     "shell.execute_reply": "2024-10-31T16:34:29.041993Z",
     "shell.execute_reply.started": "2024-10-31T16:34:29.025680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Use model_params dictionary in your Transformer model\n",
    "class Transformer(nn.Module): \n",
    "    def __init__(self, config: dict):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.d_model = config['d_model']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.d_ff = config['d_ff']\n",
    "        self.max_seq_length = config['max_seq_length']\n",
    "        self.dropout = config['dropout']\n",
    "        self.log_attention_weights = config['log_attention_weights']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(self.d_model, self.num_heads, self.d_ff, self.dropout, self.log_attention_weights) \n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(self.d_model, self.num_heads, self.d_ff, self.dropout, self.log_attention_weights) \n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(self.d_model, self.vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        device = src.device\n",
    "        src_mask = (src != 1).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 1).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout_layer(self.positional_encoding(self.embedding(src)))\n",
    "        tgt_embedded = self.dropout_layer(self.positional_encoding(self.embedding(tgt)))\n",
    "    \n",
    "        # Initialize lists to store attention weights if logging is enabled\n",
    "        enc_attentions = [] if self.log_attention_weights else None\n",
    "        dec_self_attentions = [] if self.log_attention_weights else None\n",
    "        dec_cross_attentions = [] if self.log_attention_weights else None\n",
    "    \n",
    "        # Encoder\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            if self.log_attention_weights:\n",
    "                enc_output, enc_attn = enc_layer(enc_output, src_mask)\n",
    "                enc_attentions.append(enc_attn)\n",
    "            else:\n",
    "                enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "        # Decoder\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            if self.log_attention_weights:\n",
    "                dec_output, dec_self_attn, dec_cross_attn = dec_layer(\n",
    "                    dec_output, enc_output, src_mask, tgt_mask\n",
    "                )\n",
    "                dec_self_attentions.append(dec_self_attn)\n",
    "                dec_cross_attentions.append(dec_cross_attn)\n",
    "            else:\n",
    "                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "    \n",
    "        output = self.fc(dec_output)\n",
    "    \n",
    "        if self.log_attention_weights:\n",
    "            return output, (enc_attentions, dec_self_attentions, dec_cross_attentions)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:30.783542Z",
     "iopub.status.busy": "2024-10-31T16:34:30.783162Z",
     "iopub.status.idle": "2024-10-31T16:34:30.788530Z",
     "shell.execute_reply": "2024-10-31T16:34:30.787521Z",
     "shell.execute_reply.started": "2024-10-31T16:34:30.783507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the configuration for the model\n",
    "device_info = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_gpus': torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:31.997933Z",
     "iopub.status.busy": "2024-10-31T16:34:31.997160Z",
     "iopub.status.idle": "2024-10-31T16:34:32.855409Z",
     "shell.execute_reply": "2024-10-31T16:34:32.851475Z",
     "shell.execute_reply.started": "2024-10-31T16:34:31.997877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The prepare_model function\n",
    "def prepare_model(device_info: dict, model: torch.nn.Module) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Prepare model for the current computing environment\n",
    "    Args:\n",
    "        device_info: Dictionary containing device information\n",
    "        model: PyTorch model to prepare\n",
    "    Returns:\n",
    "        Prepared model\n",
    "    \"\"\"\n",
    "    if device_info['device'] == 'cuda':\n",
    "        if device_info['num_gpus'] > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "    return model\n",
    "\n",
    "# Initialize and prepare the model\n",
    "model = Transformer(config=model_params)\n",
    "model = prepare_model(device_info, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:34:33.641203Z",
     "iopub.status.busy": "2024-10-31T16:34:33.640304Z",
     "iopub.status.idle": "2024-10-31T16:34:33.648837Z",
     "shell.execute_reply": "2024-10-31T16:34:33.647917Z",
     "shell.execute_reply.started": "2024-10-31T16:34:33.641162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(30000, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=30000, bias=True)\n",
       "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.live import Live\n",
    "from datetime import datetime\n",
    "import multiprocessing  # Import multiprocessing for setting start method\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, training_params, tokenizer):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = training_params['num_epochs']\n",
    "        self.device = training_params['device']\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=training_params['ignore_index'])\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_params['learning_rate'])\n",
    "        self.scaler = GradScaler()\n",
    "        self.clip_grad_norm = training_params.get('clip_grad_norm', None)\n",
    "        self.device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.console = Console()\n",
    "        \n",
    "        # Initialize wandb\n",
    "        if training_params.get('use_wandb', False):\n",
    "            wandb.init(\n",
    "                project=training_params.get('project_name', 'transformer-training'),\n",
    "                config=training_params\n",
    "            )\n",
    "            self.use_wandb = True\n",
    "        else:\n",
    "            self.use_wandb = False\n",
    "        \n",
    "        # Initialize TensorBoard\n",
    "        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.log_dir = os.path.join('runs', f'training_{current_time}')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.tb_writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.console.print(f\"[bold blue]TensorBoard logs will be saved to: {self.log_dir}[/bold blue]\")\n",
    "        \n",
    "    def create_metrics_table(self, metrics):\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Metric\", style=\"cyan\")\n",
    "        table.add_column(\"Value\", style=\"green\")\n",
    "        for name, value in metrics.items():\n",
    "            table.add_row(name, f\"{value:.4f}\" if isinstance(value, float) else str(value))\n",
    "        return table\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        best_loss = float('inf')\n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(\n",
    "                enumerate(self.train_loader),\n",
    "                total=len(self.train_loader),\n",
    "                desc=f\"[Epoch {epoch + 1}/{self.num_epochs}]\",\n",
    "                bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "                ncols=100\n",
    "            )\n",
    "            \n",
    "            for batch_idx, batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                # Use autocast with the new syntax\n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    loss = self.criterion(output.contiguous().view(-1, self.vocab_size), \n",
    "                                       tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                if self.clip_grad_norm:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.clip_grad_norm)\n",
    "\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                global_step += 1\n",
    "\n",
    "                # Update progress bar with colored metrics\n",
    "                progress_bar.set_postfix_str(\n",
    "                    f\"Loss: {avg_loss:.4f} | LR: {self.optimizer.param_groups[0]['lr']:.6f}\"\n",
    "                )\n",
    "\n",
    "                # Log to TensorBoard\n",
    "                self.tb_writer.add_scalar('Loss/train', avg_loss, global_step)\n",
    "                self.tb_writer.add_scalar('Learning_rate', self.optimizer.param_groups[0]['lr'], global_step)\n",
    "                \n",
    "                # Log to WandB if enabled\n",
    "                if self.use_wandb:\n",
    "                    wandb.log({\n",
    "                        'Loss/train': avg_loss,\n",
    "                        'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "                    }, step=global_step)\n",
    "            \n",
    "            # Validate after each epoch\n",
    "            val_loss = self.validate(global_step)\n",
    "            \n",
    "            # Create and display metrics table\n",
    "            metrics = {\n",
    "                \"Epoch\": f\"{epoch + 1}/{self.num_epochs}\",\n",
    "                \"Training Loss\": avg_loss,\n",
    "                \"Validation Loss\": val_loss,\n",
    "                \"Best Loss\": min(best_loss, val_loss),\n",
    "                \"Learning Rate\": self.optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "            self.console.print(self.create_metrics_table(metrics))\n",
    "            \n",
    "            # Log epoch metrics to TensorBoard\n",
    "            self.tb_writer.add_scalars('Loss/epoch', {\n",
    "                'train': avg_loss,\n",
    "                'val': val_loss\n",
    "            }, epoch)\n",
    "            \n",
    "            best_loss = min(best_loss, val_loss)\n",
    "\n",
    "    def validate(self, global_step):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(\n",
    "            self.val_loader,\n",
    "            desc=\"Validating\",\n",
    "            bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "            ncols=100\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    loss = self.criterion(output.contiguous().view(-1, self.vocab_size), \n",
    "                                       tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                avg_loss = total_loss / len(self.val_loader)\n",
    "                progress_bar.set_postfix_str(f\"Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            self.tb_writer.add_scalar('Loss/val', avg_loss, global_step)\n",
    "            \n",
    "            # Log to WandB if enabled\n",
    "            if self.use_wandb:\n",
    "                wandb.log({'Loss/val': avg_loss}, step=global_step)\n",
    "            \n",
    "            return avg_loss\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(\n",
    "            self.test_loader,\n",
    "            desc=\"Testing\",\n",
    "            bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "            ncols=100\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    loss = self.criterion(output.contiguous().view(-1, self.vocab_size), \n",
    "                                       tgt_data[:, 1:].contiguous().view(-1))\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                avg_loss = total_loss / len(self.test_loader)\n",
    "                progress_bar.set_postfix_str(f\"Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "    \n",
    "\n",
    "            # Log to TensorBoard\n",
    "            self.tb_writer.add_scalar('Loss/test', avg_loss, 0)\n",
    "            \n",
    "            # Log to WandB if enabled\n",
    "            if self.use_wandb:\n",
    "                wandb.log({'Loss/test': avg_loss})\n",
    "            \n",
    "            # Display final test results\n",
    "            metrics = {\n",
    "                \"Final Test Loss\": avg_loss\n",
    "            }\n",
    "            self.console.print(\"\\n[bold green]Test Results:[/bold green]\")\n",
    "            self.console.print(self.create_metrics_table(metrics))\n",
    "            \n",
    "        # Close TensorBoard writer\n",
    "        self.tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample training_params definition\n",
    "training_params = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'ignore_index': tokenizer.pad_token_id,  # Replace with the pad token ID used in your tokenizer\n",
    "    'vocab_size': tokenizer.vocab_size,      # Vocabulary size based on your tokenizer\n",
    "    'learning_rate': 0.001,                   # Learning rate\n",
    "    'num_epochs': 20,                         # Total epochs\n",
    "    'clip_grad_norm': 1.0,                    # Gradient clipping norm\n",
    "    'save_dir': './checkpoints',              # Directory to save checkpoints\n",
    "    'log_interval': 100,                      # Interval for logging train loss\n",
    "    'eval_interval': 1,                       # Epochs between each evaluation\n",
    "    'warmup_steps': 500,                      # Number of warmup steps for scheduler\n",
    "    'project_name': 'your_wandb_project_name', # Replace with actual project name for W&B\n",
    "    'use_wandb': True                         # Set to True if using W&B for logging\n",
    "}\n",
    "\n",
    "# Initialize and train\n",
    "trainer = Trainer(model, train_loader, val_loader, test_loader, training_params, tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:52:21.341026Z",
     "iopub.status.busy": "2024-10-31T16:52:21.340585Z",
     "iopub.status.idle": "2024-10-31T16:52:21.416835Z",
     "shell.execute_reply": "2024-10-31T16:52:21.416033Z",
     "shell.execute_reply.started": "2024-10-31T16:52:21.340987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.live import Live\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class EnhancedTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, training_params, tokenizer):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model and training parameters.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.training_params = training_params\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Basic training setup\n",
    "        self.num_epochs = training_params['num_epochs']\n",
    "        self.device = training_params['device']\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=training_params['ignore_index'])\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=training_params['learning_rate'])\n",
    "        self.scaler = GradScaler()\n",
    "        self.clip_grad_norm = training_params.get('clip_grad_norm', None)\n",
    "        self.device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Rich console setup\n",
    "        self.console = Console()\n",
    "        \n",
    "        # Training tracking\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_bleu = 0.0\n",
    "        self.best_model_path = None\n",
    "        self.global_step = 0\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        num_training_steps = len(self.train_loader) * self.num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=training_params['warmup_steps'],\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Setup logging directories\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        self.reset_metrics()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging directories and initialize loggers.\"\"\"\n",
    "        # Create checkpoint directory\n",
    "        self.save_dir = Path(self.training_params['save_dir'])\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize wandb\n",
    "        if self.training_params.get('use_wandb', False):\n",
    "            wandb.init(\n",
    "                project=self.training_params.get('project_name', 'transformer-training'),\n",
    "                config=self.training_params\n",
    "            )\n",
    "            self.use_wandb = True\n",
    "        else:\n",
    "            self.use_wandb = False\n",
    "        \n",
    "        # Initialize TensorBoard\n",
    "        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.log_dir = Path('runs') / f'training_{current_time}'\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tb_writer = SummaryWriter(log_dir=str(self.log_dir))\n",
    "        self.console.print(f\"[bold blue]TensorBoard logs will be saved to: {self.log_dir}[/bold blue]\")\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset metrics for new epoch.\"\"\"\n",
    "        self.metrics = {\n",
    "            'train_loss': 0.0,\n",
    "            'train_acc': 0.0,\n",
    "            'train_ppl': 0.0,\n",
    "            'val_loss': 0.0,\n",
    "            'val_acc': 0.0,\n",
    "            'val_ppl': 0.0,\n",
    "            'bleu': 0.0\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, output, target):\n",
    "        \"\"\"Calculate accuracy and perplexity for the batch.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Calculate accuracy\n",
    "            pred = output.argmax(dim=-1)\n",
    "            correct = (pred == target).float()\n",
    "            mask = (target != self.training_params['ignore_index']).float()\n",
    "            accuracy = (correct * mask).sum() / mask.sum()\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            loss = self.criterion(output.view(-1, self.vocab_size), target.view(-1))\n",
    "            perplexity = torch.exp(loss)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy.item(),\n",
    "                'perplexity': perplexity.item(),\n",
    "                'loss': loss.item()\n",
    "            }\n",
    "\n",
    "    def save_checkpoint(self, epoch, metrics, is_best=False):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'scaler_state_dict': self.scaler.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'best_loss': self.best_loss,\n",
    "            'best_bleu': self.best_bleu\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.save_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # Save best model if applicable\n",
    "        if is_best:\n",
    "            best_path = self.save_dir / 'best_model.pt'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            self.best_model_path = best_path\n",
    "\n",
    "    def create_metrics_table(self, metrics):\n",
    "        \"\"\"Create a rich table for displaying metrics.\"\"\"\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Metric\", style=\"cyan\")\n",
    "        table.add_column(\"Value\", style=\"green\")\n",
    "        \n",
    "        for name, value in metrics.items():\n",
    "            table.add_row(\n",
    "                name,\n",
    "                f\"{value:.4f}\" if isinstance(value, float) else str(value)\n",
    "            )\n",
    "        return table\n",
    "\n",
    "    def log_metrics(self, metrics, step):\n",
    "        \"\"\"Log metrics to both TensorBoard and W&B.\"\"\"\n",
    "        # Log to TensorBoard\n",
    "        for name, value in metrics.items():\n",
    "            self.tb_writer.add_scalar(name, value, step)\n",
    "        \n",
    "        # Log to WandB if enabled\n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.reset_metrics()\n",
    "            epoch_start_time = datetime.now()\n",
    "            \n",
    "            # Training loop\n",
    "            progress_bar = tqdm(\n",
    "                enumerate(self.train_loader),\n",
    "                total=len(self.train_loader),\n",
    "                desc=f\"[Epoch {epoch + 1}/{self.num_epochs}]\",\n",
    "                bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "                ncols=100\n",
    "            )\n",
    "            \n",
    "            for batch_idx, batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    loss = self.criterion(\n",
    "                        output.contiguous().view(-1, self.vocab_size),\n",
    "                        tgt_data[:, 1:].contiguous().view(-1)\n",
    "                    )\n",
    "\n",
    "                # Calculate batch metrics\n",
    "                batch_metrics = self.calculate_metrics(\n",
    "                    output.contiguous().view(-1, self.vocab_size),\n",
    "                    tgt_data[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "                \n",
    "                # Update running metrics\n",
    "                self.metrics['train_loss'] += batch_metrics['loss']\n",
    "                self.metrics['train_acc'] += batch_metrics['accuracy']\n",
    "                self.metrics['train_ppl'] += batch_metrics['perplexity']\n",
    "\n",
    "                # Backward pass with gradient scaling\n",
    "                self.scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping if enabled\n",
    "                if self.clip_grad_norm:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        max_norm=self.clip_grad_norm\n",
    "                    )\n",
    "\n",
    "                # Optimizer and scheduler steps\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                avg_loss = self.metrics['train_loss'] / (batch_idx + 1)\n",
    "                avg_acc = self.metrics['train_acc'] / (batch_idx + 1)\n",
    "                avg_ppl = self.metrics['train_ppl'] / (batch_idx + 1)\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                progress_bar.set_postfix_str(\n",
    "                    f\"Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f} | \"\n",
    "                    f\"PPL: {avg_ppl:.4f} | LR: {lr:.6f}\"\n",
    "                )\n",
    "                \n",
    "                # Log step metrics\n",
    "                if self.global_step % self.training_params['log_interval'] == 0:\n",
    "                    step_metrics = {\n",
    "                        'Loss/train': avg_loss,\n",
    "                        'Accuracy/train': avg_acc,\n",
    "                        'Perplexity/train': avg_ppl,\n",
    "                        'Learning_rate': lr\n",
    "                    }\n",
    "                    if self.clip_grad_norm:\n",
    "                        step_metrics['Gradient_norm'] = grad_norm.item()\n",
    "                    \n",
    "                    self.log_metrics(step_metrics, self.global_step)\n",
    "                \n",
    "                self.global_step += 1\n",
    "                \n",
    "        \n",
    "            \n",
    "            # Calculate epoch averages\n",
    "            num_batches = len(self.train_loader)\n",
    "            self.metrics['train_loss'] /= num_batches\n",
    "            self.metrics['train_acc'] /= num_batches\n",
    "            self.metrics['train_ppl'] /= num_batches\n",
    "            \n",
    "            # Validation\n",
    "            if (epoch + 1) % self.training_params['eval_interval'] == 0:\n",
    "                val_metrics = self.validate()\n",
    "                self.metrics.update(val_metrics)\n",
    "                \n",
    "                # Check for best model\n",
    "                is_best = val_metrics['val_loss'] < self.best_loss\n",
    "                if is_best:\n",
    "                    self.best_loss = val_metrics['val_loss']\n",
    "                    self.best_bleu = val_metrics['bleu']\n",
    "                \n",
    "                # Save checkpoint\n",
    "                self.save_checkpoint(epoch + 1, self.metrics, is_best)\n",
    "                \n",
    "                # Display epoch metrics\n",
    "                epoch_time = datetime.now() - epoch_start_time\n",
    "                display_metrics = {\n",
    "                    \"Epoch\": f\"{epoch + 1}/{self.num_epochs}\",\n",
    "                    \"Time\": str(epoch_time).split('.')[0],\n",
    "                    \"Train Loss\": self.metrics['train_loss'],\n",
    "                    \"Val Loss\": self.metrics['val_loss'],\n",
    "                    \"Train Acc\": self.metrics['train_acc'],\n",
    "                    \"Val Acc\": self.metrics['val_acc'],\n",
    "                    \"Train PPL\": self.metrics['train_ppl'],\n",
    "                    \"Val PPL\": self.metrics['val_ppl'],\n",
    "                    \"BLEU\": self.metrics['bleu'],\n",
    "                    \"Learning Rate\": lr\n",
    "                }\n",
    "                self.console.print(self.create_metrics_table(display_metrics))\n",
    "        \n",
    "        # Final test evaluation\n",
    "        self.test()\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validation loop.\"\"\"\n",
    "        self.model.eval()\n",
    "        val_metrics = {\n",
    "            'val_loss': 0.0,\n",
    "            'val_acc': 0.0,\n",
    "            'val_ppl': 0.0,\n",
    "            'bleu': 0.0\n",
    "        }\n",
    "        all_references = []\n",
    "        all_hypotheses = []\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.val_loader,\n",
    "            desc=\"Validating\",\n",
    "            bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "            ncols=100\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    batch_metrics = self.calculate_metrics(\n",
    "                        output.contiguous().view(-1, self.vocab_size),\n",
    "                        tgt_data[:, 1:].contiguous().view(-1)\n",
    "                    )\n",
    "                    \n",
    "                    # Update running metrics\n",
    "                    val_metrics['val_loss'] += batch_metrics['loss']\n",
    "                    val_metrics['val_acc'] += batch_metrics['accuracy']\n",
    "                    val_metrics['val_ppl'] += batch_metrics['perplexity']\n",
    "                    \n",
    "                    # Collect translations for BLEU score\n",
    "                    predictions = output.argmax(dim=-1)\n",
    "                    for pred, target in zip(predictions, tgt_data[:, 1:]):\n",
    "                        pred_tokens = [token.item() for token in pred if token.item() not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]]\n",
    "                        target_tokens = [token.item() for token in target if token.item() not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]]\n",
    "                        all_hypotheses.append(pred_tokens)\n",
    "                        all_references.append([target_tokens])\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(self.val_loader)\n",
    "        val_metrics['val_loss'] /= num_batches\n",
    "        val_metrics['val_acc'] /= num_batches\n",
    "        val_metrics['val_ppl'] /= num_batches\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        val_metrics['bleu'] = corpus_bleu(all_references, all_hypotheses) * 100\n",
    "        \n",
    "        return val_metrics\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test loop.\"\"\"\n",
    "        self.model.eval()\n",
    "        test_metrics = {\n",
    "            'test_loss': 0.0,\n",
    "            'test_acc': 0.0,\n",
    "            'test_ppl': 0.0,\n",
    "            'test_bleu': 0.0\n",
    "        }\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            self.test_loader,\n",
    "            desc=\"Testing\",\n",
    "            bar_format=\"{desc}: {percentage:3.0f}%|{bar:30}{r_bar}\",\n",
    "            ncols=100\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                src_data = batch['src'].to(self.device)\n",
    "                tgt_data = batch['tgt'].to(self.device)\n",
    "\n",
    "                with autocast(device_type=self.device_type):\n",
    "                    output = self.model(src_data, tgt_data[:, :-1])\n",
    "                    batch_metrics = self.calculate_metrics(\n",
    "                        output.contiguous().view(-1, self.vocab_size),\n",
    "                        tgt_data[:, 1:].contiguous().view(-1)\n",
    "                    )\n",
    "                    \n",
    "                    test_metrics['test_loss'] += batch_metrics['loss']\n",
    "                    test_metrics['test_acc'] += batch_metrics['accuracy']\n",
    "                    test_metrics['test_ppl'] += batch_metrics['perplexity']\n",
    "\n",
    "                    # Collect translations for BLEU score\n",
    "                    predictions = output.argmax(dim=-1)\n",
    "                    references = []\n",
    "                    hypotheses = []\n",
    "                    for pred, target in zip(predictions, tgt_data[:, 1:]):\n",
    "                        pred_tokens = [token.item() for token in pred if token.item() not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]]\n",
    "                        target_tokens = [token.item() for token in target if token.item() not in [self.tokenizer.pad_token_id, self.tokenizer.eos_token_id]]\n",
    "                        hypotheses.append(pred_tokens)\n",
    "                        references.append([target_tokens])\n",
    "\n",
    "        # Calculate averages\n",
    "        num_batches = len(self.test_loader)\n",
    "        for key in test_metrics:\n",
    "            test_metrics[key] /= num_batches\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        test_metrics['test_bleu'] = corpus_bleu(references, hypotheses) * 100\n",
    "\n",
    "        # Log final test metrics\n",
    "        self.log_metrics({\n",
    "            'Loss/test': test_metrics['test_loss'],\n",
    "            'Accuracy/test': test_metrics['test_acc'],\n",
    "            'Perplexity/test': test_metrics['test_ppl'],\n",
    "            'BLEU/test': test_metrics['test_bleu']\n",
    "        }, self.global_step)\n",
    "\n",
    "        # Display final test results\n",
    "        final_metrics = {\n",
    "            \"Final Test Loss\": test_metrics['test_loss'],\n",
    "            \"Final Test Accuracy\": test_metrics['test_acc'],\n",
    "            \"Final Test Perplexity\": test_metrics['test_ppl'],\n",
    "            \"Final Test BLEU\": test_metrics['test_bleu']\n",
    "        }\n",
    "        \n",
    "        self.console.print(\"\\n[bold green]Test Results:[/bold green]\")\n",
    "        self.console.print(self.create_metrics_table(final_metrics))\n",
    "        \n",
    "        # Close TensorBoard writer\n",
    "        self.tb_writer.close()\n",
    "\n",
    "    def generate_translation(self, src_text, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate translation for a given source text.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize input text\n",
    "        src_tokens = self.tokenizer(src_text, return_tensors=\"pt\", padding=True)\n",
    "        src_tokens = src_tokens['input_ids'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type=self.device_type):\n",
    "                # Initialize target sequence with start token\n",
    "                tgt = torch.tensor([[self.tokenizer.bos_token_id]]).to(self.device)\n",
    "                \n",
    "                for _ in range(max_length):\n",
    "                    # Generate next token\n",
    "                    output = self.model(src_tokens, tgt)\n",
    "                    next_token = output[:, -1:].argmax(dim=-1)\n",
    "                    tgt = torch.cat([tgt, next_token], dim=1)\n",
    "                    \n",
    "                    # Stop if end token is generated\n",
    "                    if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "        \n",
    "        # Convert tokens to text\n",
    "        generated_tokens = tgt[0].tolist()\n",
    "        translated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return translated_text\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Load a saved checkpoint.\n",
    "        \"\"\"\n",
    "        self.console.print(f\"[bold blue]Loading checkpoint from {checkpoint_path}[/bold blue]\")\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        # Load model and training states\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        \n",
    "        # Load best metrics\n",
    "        self.best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "        self.best_bleu = checkpoint.get('best_bleu', 0.0)\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Cleanup resources and save final artifacts.\n",
    "        \"\"\"\n",
    "        # Close TensorBoard writer if it exists\n",
    "        if hasattr(self, 'tb_writer'):\n",
    "            self.tb_writer.close()\n",
    "        \n",
    "        # Finish wandb run if it was used\n",
    "        if self.use_wandb:\n",
    "            wandb.finish()\n",
    "        \n",
    "        # Save final model state\n",
    "        final_checkpoint = {\n",
    "            'epoch': self.num_epochs,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'scaler_state_dict': self.scaler.state_dict(),\n",
    "            'best_loss': self.best_loss,\n",
    "            'best_bleu': self.best_bleu,\n",
    "            'final_metrics': self.metrics\n",
    "        }\n",
    "        \n",
    "        final_path = self.save_dir / 'final_model.pt'\n",
    "        torch.save(final_checkpoint, final_path)\n",
    "        self.console.print(f\"[bold green]Final model saved to {final_path}[/bold green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T16:53:51.797264Z",
     "iopub.status.busy": "2024-10-31T16:53:51.796870Z",
     "iopub.status.idle": "2024-10-31T16:54:11.991324Z",
     "shell.execute_reply": "2024-10-31T16:54:11.989879Z",
     "shell.execute_reply.started": "2024-10-31T16:53:51.797228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ro526pb4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy/train</td><td>▁</td></tr><tr><td>Gradient_norm</td><td>▁</td></tr><tr><td>Learning_rate</td><td>▁</td></tr><tr><td>Loss/train</td><td>▁</td></tr><tr><td>Perplexity/train</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy/train</td><td>0.04142</td></tr><tr><td>Gradient_norm</td><td>2.47832</td></tr><tr><td>Learning_rate</td><td>0.0</td></tr><tr><td>Loss/train</td><td>8.70312</td></tr><tr><td>Perplexity/train</td><td>6020</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cryptic-sorcery-36</strong> at: <a href='https://wandb.ai/learning_/your_wandb_project_name/runs/ro526pb4' target=\"_blank\">https://wandb.ai/learning_/your_wandb_project_name/runs/ro526pb4</a><br/> View project at: <a href='https://wandb.ai/learning_/your_wandb_project_name' target=\"_blank\">https://wandb.ai/learning_/your_wandb_project_name</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241031_165222-ro526pb4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ro526pb4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241031_165351-pjot1jek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/learning_/your_wandb_project_name/runs/pjot1jek' target=\"_blank\">toxic-candle-37</a></strong> to <a href='https://wandb.ai/learning_/your_wandb_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/learning_/your_wandb_project_name' target=\"_blank\">https://wandb.ai/learning_/your_wandb_project_name</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/learning_/your_wandb_project_name/runs/pjot1jek' target=\"_blank\">https://wandb.ai/learning_/your_wandb_project_name/runs/pjot1jek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">TensorBoard logs will be saved to: runs/training_20241031_165355</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mTensorBoard logs will be saved to: runs/training_20241031_165355\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20]: 100%|██████████████████████████████| 1/1 [00:00<00:00,  4.20it/s, Loss: 8.3672 | Acc: \n",
      "Validating: 100%|██████████████████████████████| 1/1 [00:00<00:00, 25.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metric        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Value      </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Epoch         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 1/20       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Time          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0:00:03    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Loss    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 8.3672     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Loss      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 9.2109     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Acc     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0947     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Acc       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0588     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train PPL     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 4304.0000  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val PPL       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 10005.9736 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> BLEU          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 35.9304    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Learning Rate </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0000     </span>│\n",
       "└───────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mMetric       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mValue     \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mEpoch        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1/20      \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTime         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0:00:03   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m8.3672    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m9.2109    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0947    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Acc      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0588    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain PPL    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m4304.0000 \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal PPL      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m10005.9736\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mBLEU         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m35.9304   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLearning Rate\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0000    \u001b[0m\u001b[32m \u001b[0m│\n",
       "└───────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/20]: 100%|██████████████████████████████| 1/1 [00:00<00:00,  4.28it/s, Loss: 8.2422 | Acc: \n",
      "Validating: 100%|██████████████████████████████| 1/1 [00:00<00:00, 25.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metric        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Value      </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Epoch         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 2/20       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Time          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0:00:02    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Loss    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 8.2422     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Loss      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 9.2109     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Acc     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0947     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Acc       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0588     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train PPL     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 3798.0000  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val PPL       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 10005.9736 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> BLEU          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 35.8559    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Learning Rate </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0000     </span>│\n",
       "└───────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mMetric       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mValue     \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mEpoch        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m2/20      \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTime         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0:00:02   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m8.2422    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m9.2109    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0947    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Acc      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0588    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain PPL    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3798.0000 \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal PPL      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m10005.9736\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mBLEU         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m35.8559   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLearning Rate\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0000    \u001b[0m\u001b[32m \u001b[0m│\n",
       "└───────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/20]: 100%|██████████████████████████████| 1/1 [00:00<00:00,  4.54it/s, Loss: 8.2344 | Acc: \n",
      "Validating: 100%|██████████████████████████████| 1/1 [00:00<00:00, 24.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metric        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Value     </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Epoch         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 3/20      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Time          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0:00:08   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Loss    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 8.2344    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Loss      </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 9.2096    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train Acc     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0947    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val Acc       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0588    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Train PPL     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 3768.0000 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Val PPL       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 9992.9561 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> BLEU          </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 35.7093   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Learning Rate </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.0000    </span>│\n",
       "└───────────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mMetric       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mValue    \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mEpoch        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3/20     \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTime         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0:00:08  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m8.2344   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m9.2096   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain Acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0947   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal Acc      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0588   \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTrain PPL    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3768.0000\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mVal PPL      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m9992.9561\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mBLEU         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m35.7093  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLearning Rate\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m0.0000   \u001b[0m\u001b[32m \u001b[0m│\n",
       "└───────────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/20]: 100%|██████████████████████████████| 1/1 [00:00<00:00,  4.54it/s, Loss: 8.2188 | Acc: \n",
      "Validating: 100%|██████████████████████████████| 1/1 [00:00<00:00, 25.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EnhancedTrainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Generate translations\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# translation = trainer.generate_translation(\"Your source text here\")\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Cleanup when done\u001b[39;00m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "Cell \u001b[0;32mIn[33], line 266\u001b[0m, in \u001b[0;36mEnhancedTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_bleu \u001b[38;5;241m=\u001b[39m val_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_best\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Display epoch metrics\u001b[39;00m\n\u001b[1;32m    269\u001b[0m epoch_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m epoch_start_time\n",
      "Cell \u001b[0;32mIn[33], line 132\u001b[0m, in \u001b[0;36mEnhancedTrainer.save_checkpoint\u001b[0;34m(self, epoch, metrics, is_best)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Save regular checkpoint\u001b[39;00m\n\u001b[1;32m    131\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Save best model if applicable\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_best:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sample training_params definition\n",
    "training_params = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'ignore_index': tokenizer.pad_token_id,  # Replace with the pad token ID used in your tokenizer\n",
    "    'vocab_size': tokenizer.vocab_size,      # Vocabulary size based on your tokenizer\n",
    "    'learning_rate': 0.001,                   # Learning rate\n",
    "    'num_epochs': 20,                         # Total epochs\n",
    "    'clip_grad_norm': 1.0,                    # Gradient clipping norm\n",
    "    'save_dir': './checkpoints',              # Directory to save checkpoints\n",
    "    'log_interval': 100,                      # Interval for logging train loss\n",
    "    'eval_interval': 1,                       # Epochs between each evaluation\n",
    "    'warmup_steps': 500,                      # Number of warmup steps for scheduler\n",
    "    'project_name': 'your_wandb_project_name', # Replace with actual project name for W&B\n",
    "    'use_wandb': True                         # Set to True if using W&B for logging\n",
    "}\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = EnhancedTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    training_params=training_params,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Generate translations\n",
    "# translation = trainer.generate_translation(\"Your source text here\")\n",
    "\n",
    "# Load a checkpoint\n",
    "# trainer.load_checkpoint(\"path/to/checkpoint.pt\")\n",
    "\n",
    "# Cleanup when done\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = '/kaggle/working/checkpoints/final_model.pt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"The checkpoint file exists.\")\n",
    "else:\n",
    "    print(\"The checkpoint file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Assuming your model is already defined\n",
    "# model = YourModelClass(...)  # Define your model here\n",
    "\n",
    "checkpoint_path = '/kaggle/working/checkpoints/final_model.pt'\n",
    "\n",
    "# Check if the checkpoint file exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=True)  # Use weights_only=True for safety\n",
    "\n",
    "    # Load the model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded successfully for inference from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(f\"Checkpoint file does not exist at {checkpoint_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Specify the log directory\n",
    "log_dir = '/kaggle/working/runs/training_20241030_122948/events.out.tfevents.1730291388.e073326608c4.30.0'  # This points to your log directory\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir $log_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install arabic-reshaper python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Noto Nastaliq Urdu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "fm.fontManager.ttflist = []  # Clear the list\n",
    "for font in fm.findSystemFonts():\n",
    "    try:\n",
    "        fm.fontManager.addfont(font)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.font_manager as fm\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, tokenizer, font_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer with optional font path for Urdu text\n",
    "        If font_path is None, will try to use Noto Naskh Arabic or similar fonts\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.layer_names = {\n",
    "            'encoder': 'Encoder Layer',\n",
    "            'decoder_self': 'Decoder Self-Attention Layer',\n",
    "            'decoder_cross': 'Decoder Cross-Attention Layer'\n",
    "        }\n",
    "        \n",
    "        # Set up font for Urdu text\n",
    "        if font_path:\n",
    "            self.font_prop = fm.FontProperties(fname=font_path)\n",
    "        else:\n",
    "            # Try to find a suitable Arabic/Urdu font\n",
    "            font_names = ['Noto Naskh Arabic', 'Arabic Typesetting', 'Traditional Arabic']\n",
    "            for font_name in font_names:\n",
    "                if any(f.name == font_name for f in fm.fontManager.ttflist):\n",
    "                    self.font_prop = fm.FontProperties(family=font_name)\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Warning: No suitable Arabic/Urdu font found. Text may not display correctly.\")\n",
    "                self.font_prop = None\n",
    "\n",
    "    def _process_urdu_text(self, text: str) -> str:\n",
    "        \"\"\"Process Urdu text for proper display\"\"\"\n",
    "        # Reshape Arabic/Urdu text\n",
    "        reshaped_text = arabic_reshaper.reshape(text)\n",
    "        # Handle right-to-left text\n",
    "        bidi_text = get_display(reshaped_text)\n",
    "        return bidi_text\n",
    "\n",
    "    def _get_tokens_from_ids(self, token_ids: torch.Tensor, batch_idx: int = 0) -> List[str]:\n",
    "        \"\"\"Convert token IDs to tokens, handling batched input\"\"\"\n",
    "        # Remove padding tokens\n",
    "        mask = token_ids[batch_idx] != self.tokenizer.pad_token_id\n",
    "        valid_ids = token_ids[batch_idx][mask]\n",
    "        # Decode individual tokens and process for Urdu display\n",
    "        tokens = [self._process_urdu_text(self.tokenizer.decode([id.item()], skip_special_tokens=False))\n",
    "                 for id in valid_ids]\n",
    "        return tokens\n",
    "\n",
    "    def plot_attention_weights(self, \n",
    "                             attention_weights: Tuple[List[torch.Tensor], ...],\n",
    "                             src_ids: torch.Tensor,\n",
    "                             tgt_ids: torch.Tensor,\n",
    "                             batch_idx: int = 0,\n",
    "                             layer_idx: int = 0,\n",
    "                             head_idx: int = 0,\n",
    "                             save_path: Optional[str] = None,\n",
    "                             figsize=(20, 15)):\n",
    "        \"\"\"Plot attention weights with proper Urdu text support\"\"\"\n",
    "        enc_attentions, dec_self_attentions, dec_cross_attentions = attention_weights\n",
    "        \n",
    "        # Get tokens from IDs for the specified batch\n",
    "        src_tokens = self._get_tokens_from_ids(src_ids, batch_idx)\n",
    "        tgt_tokens = self._get_tokens_from_ids(tgt_ids, batch_idx)\n",
    "\n",
    "        # Create figure with larger size to accommodate Urdu text\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(2, 2, figure=fig)\n",
    "        \n",
    "        # 1. Encoder Self-Attention\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        self._plot_attention_map(\n",
    "            attention_weights=enc_attentions[layer_idx][batch_idx, head_idx, :len(src_tokens), :len(src_tokens)].detach().cpu(),\n",
    "            x_labels=src_tokens,\n",
    "            y_labels=src_tokens,\n",
    "            title=f\"{self.layer_names['encoder']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            ax=ax1\n",
    "        )\n",
    "\n",
    "        # 2. Decoder Self-Attention\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        self._plot_attention_map(\n",
    "            attention_weights=dec_self_attentions[layer_idx][batch_idx, head_idx, :len(tgt_tokens), :len(tgt_tokens)].detach().cpu(),\n",
    "            x_labels=tgt_tokens,\n",
    "            y_labels=tgt_tokens,\n",
    "            title=f\"{self.layer_names['decoder_self']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            ax=ax2\n",
    "        )\n",
    "\n",
    "        # 3. Decoder Cross-Attention\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        self._plot_attention_map(\n",
    "            attention_weights=dec_cross_attentions[layer_idx][batch_idx, head_idx, :len(tgt_tokens), :len(src_tokens)].detach().cpu(),\n",
    "            x_labels=src_tokens,\n",
    "            y_labels=tgt_tokens,\n",
    "            title=f\"{self.layer_names['decoder_cross']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            ax=ax3\n",
    "        )\n",
    "\n",
    "        plt.tight_layout(pad=3.0)  # Added extra padding for Urdu text\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_attention_map(self,\n",
    "                           attention_weights: torch.Tensor,\n",
    "                           x_labels: List[str],\n",
    "                           y_labels: List[str],\n",
    "                           title: str,\n",
    "                           ax: plt.Axes):\n",
    "        \"\"\"Plot a single attention map with Urdu text support\"\"\"\n",
    "        sns.heatmap(\n",
    "            attention_weights,\n",
    "            xticklabels=x_labels,\n",
    "            yticklabels=y_labels,\n",
    "            cmap='YlOrRd',\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': 'Attention Weight'}\n",
    "        )\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Key Tokens')\n",
    "        ax.set_ylabel('Query Tokens')\n",
    "        \n",
    "        # Adjust label properties for better Urdu text display\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontproperties=self.font_prop)\n",
    "        plt.setp(ax.get_yticklabels(), rotation=0, fontproperties=self.font_prop)\n",
    "        \n",
    "        # Increase spacing for tick labels\n",
    "        ax.tick_params(axis='x', pad=10)\n",
    "        ax.tick_params(axis='y', pad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = ['Noto Nastaliq Urdu', 'Arial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualizer = AttentionVisualizer(tokenizer)\n",
    "\n",
    "visualizer.plot_attention_weights(\n",
    "    attention_weights=attention_weights,\n",
    "    src_ids=src_data,\n",
    "    tgt_ids=tgt_data,\n",
    "    batch_idx=0,\n",
    "    layer_idx=0,  # Change to view different layers\n",
    "    head_idx=0    # Change to view different heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Ensure proper display in the notebook\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "\n",
    "class AttentionVisualizerPlotly:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.layer_names = {\n",
    "            'encoder': 'Encoder Layer',\n",
    "            'decoder_self': 'Decoder Self-Attention Layer',\n",
    "            'decoder_cross': 'Decoder Cross-Attention Layer'\n",
    "        }\n",
    "\n",
    "    def _get_tokens_from_ids(self, token_ids: torch.Tensor, batch_idx: int = 0) -> List[str]:\n",
    "        \"\"\"Convert token IDs to tokens, handling batched input\"\"\"\n",
    "        # Remove padding tokens\n",
    "        mask = token_ids[batch_idx] != self.tokenizer.pad_token_id\n",
    "        valid_ids = token_ids[batch_idx][mask]\n",
    "        # Decode individual tokens\n",
    "        tokens = [self.tokenizer.decode([id.item()], skip_special_tokens=False) \n",
    "                  for id in valid_ids]\n",
    "        return tokens\n",
    "\n",
    "    def plot_attention_weights(self, \n",
    "                               attention_weights: Tuple[List[torch.Tensor], ...],\n",
    "                               src_ids: torch.Tensor,\n",
    "                               tgt_ids: torch.Tensor,\n",
    "                               batch_idx: int = 0,\n",
    "                               layer_idx: int = 0,\n",
    "                               head_idx: int = 0,\n",
    "                               save_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Plot attention weights using token IDs directly with Plotly.\n",
    "        \n",
    "        Args:\n",
    "            attention_weights: Tuple of (encoder_attentions, decoder_self_attentions, decoder_cross_attentions)\n",
    "            src_ids: Source token IDs [batch_size, src_len]\n",
    "            tgt_ids: Target token IDs [batch_size, tgt_len]\n",
    "            batch_idx: Which batch item to visualize\n",
    "            layer_idx: Index of the layer to visualize\n",
    "            head_idx: Index of the attention head to visualize\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        enc_attentions, dec_self_attentions, dec_cross_attentions = attention_weights\n",
    "        \n",
    "        # Get tokens from IDs for the specified batch\n",
    "        src_tokens = self._get_tokens_from_ids(src_ids, batch_idx)\n",
    "        tgt_tokens = self._get_tokens_from_ids(tgt_ids, batch_idx)\n",
    "\n",
    "        # 1. Encoder Self-Attention\n",
    "        fig = self._plot_attention_map(\n",
    "            attention_weights=enc_attentions[layer_idx][batch_idx, head_idx, :len(src_tokens), :len(src_tokens)].detach().cpu(),\n",
    "            x_labels=src_tokens,\n",
    "            y_labels=src_tokens,\n",
    "            title=f\"{self.layer_names['encoder']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            fig=go.Figure()\n",
    "        )\n",
    "\n",
    "        # 2. Decoder Self-Attention\n",
    "        fig = self._plot_attention_map(\n",
    "            attention_weights=dec_self_attentions[layer_idx][batch_idx, head_idx, :len(tgt_tokens), :len(tgt_tokens)].detach().cpu(),\n",
    "            x_labels=tgt_tokens,\n",
    "            y_labels=tgt_tokens,\n",
    "            title=f\"{self.layer_names['decoder_self']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            fig=fig\n",
    "        )\n",
    "\n",
    "        # 3. Decoder Cross-Attention\n",
    "        fig = self._plot_attention_map(\n",
    "            attention_weights=dec_cross_attentions[layer_idx][batch_idx, head_idx, :len(tgt_tokens), :len(src_tokens)].detach().cpu(),\n",
    "            x_labels=src_tokens,\n",
    "            y_labels=tgt_tokens,\n",
    "            title=f\"{self.layer_names['decoder_cross']} {layer_idx+1}\\nHead {head_idx+1}\",\n",
    "            fig=fig\n",
    "        )\n",
    "\n",
    "        if save_path:\n",
    "            fig.write_image(save_path)\n",
    "        fig.show()\n",
    "\n",
    "    def _plot_attention_map(self,\n",
    "                            attention_weights: torch.Tensor,\n",
    "                            x_labels: List[str],\n",
    "                            y_labels: List[str],\n",
    "                            title: str,\n",
    "                            fig: go.Figure) -> go.Figure:\n",
    "        \"\"\"Plot a single attention map with Plotly.\"\"\"\n",
    "        heatmap = go.Heatmap(\n",
    "            z=attention_weights.numpy(),\n",
    "            x=x_labels,\n",
    "            y=y_labels,\n",
    "            colorscale='YlOrRd',\n",
    "            colorbar=dict(title='Attention Weight')\n",
    "        )\n",
    "\n",
    "        fig.add_trace(heatmap)\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            xaxis_title='Key Tokens',\n",
    "            yaxis_title='Query Tokens',\n",
    "            xaxis_tickangle=45\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the necessary attention weights, source and target token IDs, and a tokenizer.\n",
    "visualizer = AttentionVisualizerPlotly(tokenizer)\n",
    "visualizer.plot_attention_weights(attention_weights, src_data, tgt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualizer = AttentionVisualizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # After your training loop where you have attention_weights\n",
    "# visualizer.plot_attention_weights(\n",
    "#     attention_weights=attention_weights,\n",
    "#     src_text= src_data,\n",
    "#     tgt_text=tgt_data,\n",
    "#     layer_idx=0,  # Choose layer to visualize\n",
    "#     head_idx=0,   # Choose head to visualize\n",
    "#     save_path=\"attention_plot.png\"  # Optional\n",
    "# )\n",
    "\n",
    "visualizer.plot_attention_weights(\n",
    "    attention_weights=attention_weights,\n",
    "    src_ids=src_data,\n",
    "    tgt_ids=tgt_data,\n",
    "    batch_idx=0,\n",
    "    layer_idx=0,  # Change to view different layers\n",
    "    head_idx=0    # Change to view different heads\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the configuration class for training hyperparameters\n",
    "class TrainConfig:\n",
    "    def __init__(self, num_epochs=10, lr=1e-4, batch_size=32, device=None, criterion=None, optimizer_cls=None):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = criterion if criterion else nn.CrossEntropyLoss(ignore_index=1)\n",
    "        self.optimizer_cls = optimizer_cls if optimizer_cls else optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from sacrebleu.metrics import BLEU\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    base_dir: str = \"mt_training\"\n",
    "    model_name: str = \"urdu_english_transformer\"\n",
    "    num_epochs: int = 100\n",
    "    batch_size: int = 32\n",
    "    grad_accum_steps: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 4000\n",
    "    eval_steps: int = 1000\n",
    "    save_steps: int = 2000\n",
    "    early_stopping_patience: int = 5\n",
    "    max_seq_length: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    min_learning_rate: float = 1e-5\n",
    "    weight_decay: float = 0.01\n",
    "    log_interval: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MTTrainer:\n",
    "    def __init__(self, config: TrainingConfig, model, tokenizer):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.setup_directories()\n",
    "        self.setup_logging()\n",
    "        self.setup_distributed()\n",
    "        self.setup_model(model, tokenizer)\n",
    "        self.setup_tracking()\n",
    "        \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories for logs, checkpoints, etc.\"\"\"\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.run_dir = Path(self.config.base_dir) / f\"run_{self.timestamp}\"\n",
    "        \n",
    "        self.dirs = {\n",
    "            'checkpoints': self.run_dir / 'checkpoints',\n",
    "            'logs': self.run_dir / 'logs',\n",
    "            'tensorboard': self.run_dir / 'tensorboard',\n",
    "            'predictions': self.run_dir / 'predictions'\n",
    "        }\n",
    "        \n",
    "        for dir_path in self.dirs.values():\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging to file and console\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.dirs['logs'] / 'training.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_distributed(self):\n",
    "        \"\"\"Initialize distributed training\"\"\"\n",
    "        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "        self.world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "        \n",
    "        if self.world_size > 1:\n",
    "            torch.cuda.set_device(self.local_rank)\n",
    "            dist.init_process_group(backend='nccl')\n",
    "            self.logger.info(f\"Initialized distributed training with world size {self.world_size}\")\n",
    "\n",
    "    def setup_model(self, model, tokenizer):\n",
    "        \"\"\"Setup model, optimizer, and scheduler\"\"\"\n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        if self.world_size > 1:\n",
    "            self.model = DDP(self.model, device_ids=[self.local_rank])\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Calculate steps per epoch for scheduling\n",
    "        self.steps_per_epoch = None  # Set later in train()\n",
    "\n",
    "        self.scheduler = None  # Set in train() after steps_per_epoch is known\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None\n",
    "        \n",
    "    def setup_tracking(self):\n",
    "        \"\"\"Initialize wandb and tensorboard\"\"\"\n",
    "        if self.local_rank == 0:\n",
    "            self.writer = SummaryWriter(self.dirs['tensorboard'])\n",
    "            wandb.init(\n",
    "                project=\"urdu_english_mt\",\n",
    "                name=f\"run_{self.timestamp}\",\n",
    "                config=self.config.__dict__,\n",
    "                dir=str(self.run_dir)\n",
    "            )\n",
    "            \n",
    "            # Log model architecture\n",
    "            self.writer.add_text(\n",
    "                'Model/Architecture', \n",
    "                str(self.model), \n",
    "                0\n",
    "            )\n",
    "\n",
    "    def get_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup\"\"\"\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            total_steps=self.config.num_epochs * self.steps_per_epoch,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos',\n",
    "            final_div_factor=self.config.learning_rate / self.config.min_learning_rate\n",
    "        )\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch: int):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_metrics = {\n",
    "            'attention_entropy': [],\n",
    "            'grad_norm': [],\n",
    "            'learning_rate': []\n",
    "        }\n",
    "        \n",
    "        for step, batch in enumerate(train_loader):\n",
    "            # Move batch to the correct device\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(self.device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):\n",
    "                loss, metrics = self.training_step(batch)\n",
    "                \n",
    "            # Gradient accumulation\n",
    "            scaled_loss = self.scaler.scale(loss / self.config.grad_accum_steps) if self.scaler else loss / self.config.grad_accum_steps\n",
    "            scaled_loss.backward() if self.scaler else loss.backward()\n",
    "            \n",
    "            if (step + 1) % self.config.grad_accum_steps == 0:\n",
    "                if self.scaler:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    self.scheduler.step()  # Scheduler step after optimizer\n",
    "                else:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()  # Scheduler step after optimizer\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Update metrics\n",
    "                epoch_metrics['grad_norm'].append(grad_norm.item())\n",
    "                epoch_metrics['learning_rate'].append(self.scheduler.get_last_lr()[0])\n",
    "                \n",
    "                if step % self.config.log_interval == 0:\n",
    "                    self.log_step(epoch, step, loss.item(), metrics, epoch_metrics)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Save checkpoint if needed\n",
    "            global_step = epoch * len(train_loader) + step\n",
    "            if global_step % self.config.save_steps == 0:\n",
    "                self.save_checkpoint(epoch, global_step, metrics)\n",
    "                \n",
    "        return epoch_loss / len(train_loader), epoch_metrics\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        source_ids = batch['src']\n",
    "        target_ids = batch['tgt']\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=source_ids,\n",
    "            decoder_input_ids=target_ids[:, :-1],\n",
    "            labels=target_ids[:, 1:],\n",
    "            return_dict=True,\n",
    "            output_attentions=True  # Ensure attention weights are returned\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_step_metrics(outputs, target_ids)\n",
    "        \n",
    "        return outputs.loss, metrics\n",
    "\n",
    "    def calculate_step_metrics(self, outputs, target_ids):\n",
    "        \"\"\"Calculate various metrics for the current step\"\"\"\n",
    "        with torch.no_grad():\n",
    "            metrics = {\n",
    "                'attention_entropy': self.calculate_attention_entropy(outputs.attentions),\n",
    "                'accuracy': self.calculate_accuracy(outputs.logits, target_ids),\n",
    "                'perplexity': torch.exp(outputs.loss).item()\n",
    "            }\n",
    "        return metrics\n",
    "\n",
    "    def calculate_attention_entropy(self, attention_maps):\n",
    "        \"\"\"Calculate entropy of attention distributions\"\"\"\n",
    "        if attention_maps is None:\n",
    "            return 0.0\n",
    "            \n",
    "        entropy = 0\n",
    "        for attn in attention_maps:\n",
    "            probs = torch.softmax(attn, dim=-1)\n",
    "            entropy -= torch.mean(torch.sum(probs * torch.log(probs + 1e-9), dim=-1))\n",
    "        return entropy.item() / len(attention_maps)\n",
    "\n",
    "    def calculate_accuracy(self, logits, labels):\n",
    "        \"\"\"Calculate token-level accuracy\"\"\"\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        mask = labels != self.tokenizer.pad_token_id\n",
    "        correct = (predictions == labels) & mask\n",
    "        return torch.sum(correct).item() / torch.sum(mask).item()\n",
    "\n",
    "    def evaluate(self, valid_loader):\n",
    "        \"\"\"Evaluate on the validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        bleu = BLEU()\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                # Move batch to the correct device\n",
    "                for key in batch:\n",
    "                    batch[key] = batch[key].to(self.device)\n",
    "                \n",
    "                loss, metrics = self.training_step(batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions and references for BLEU\n",
    "                preds = self.model.generate(batch['src'])\n",
    "                decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "                decoded_labels = self.tokenizer.batch_decode(batch['tgt'], skip_special_tokens=True)\n",
    "                \n",
    "                predictions.extend([self.tokenizer.tokenize(pred) for pred in decoded_preds])\n",
    "                references.extend([[self.tokenizer.tokenize(ref)] for ref in decoded_labels])\n",
    "        \n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        bleu_score = bleu.corpus_score(predictions, references).score\n",
    "        \n",
    "        return avg_val_loss, bleu_score\n",
    "\n",
    "    def save_checkpoint(self, epoch, step, metrics):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_path = self.dirs['checkpoints'] / f\"checkpoint_epoch_{epoch}_step_{step}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        self.logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    def log_step(self, epoch, step, loss, metrics, epoch_metrics):\n",
    "        \"\"\"Log training step information\"\"\"\n",
    "        self.logger.info(f\"Epoch {epoch} Step {step}: Loss {loss:.4f}, \"\n",
    "                         f\"Attention Entropy: {metrics['attention_entropy']:.4f}, \"\n",
    "                         f\"Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                         f\"Perplexity: {metrics['perplexity']:.4f}\")\n",
    "        \n",
    "        if self.local_rank == 0:\n",
    "            self.writer.add_scalar('Loss/train', loss, epoch * self.steps_per_epoch + step)\n",
    "            self.writer.add_scalar('Attention Entropy', metrics['attention_entropy'], epoch * self.steps_per_epoch + step)\n",
    "            self.writer.add_scalar('Accuracy', metrics['accuracy'], epoch * self.steps_per_epoch + step)\n",
    "            self.writer.add_scalar('Perplexity', metrics['perplexity'], epoch * self.steps_per_epoch + step)\n",
    "            self.writer.add_scalar('Learning Rate', epoch_metrics['learning_rate'][-1], epoch * self.steps_per_epoch + step)\n",
    "            self.writer.add_scalar('Grad Norm', epoch_metrics['grad_norm'][-1], epoch * self.steps_per_epoch + step)\n",
    "\n",
    "    def train(self, train_loader, valid_loader=None):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        self.steps_per_epoch = len(train_loader) // self.config.grad_accum_steps\n",
    "        self.scheduler = self.get_scheduler()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_loss, epoch_metrics = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            if self.local_rank == 0:\n",
    "                self.logger.info(f\"Epoch {epoch}: Average Training Loss {epoch_loss:.4f}\")\n",
    "                self.writer.add_scalar('Loss/train_epoch', epoch_loss, epoch)\n",
    "            \n",
    "            # Validate after each epoch if validation set is provided\n",
    "            if valid_loader is not None:\n",
    "                val_loss, bleu_score = self.evaluate(valid_loader)\n",
    "                \n",
    "                if self.local_rank == 0:\n",
    "                    self.logger.info(f\"Epoch {epoch}: Validation Loss {val_loss:.4f}, BLEU Score: {bleu_score:.4f}\")\n",
    "                    self.writer.add_scalar('Loss/val_epoch', val_loss, epoch)\n",
    "                    self.writer.add_scalar('BLEU', bleu_score, epoch)\n",
    "            \n",
    "            # Save model checkpoint after each epoch\n",
    "            if self.local_rank == 0:\n",
    "                self.save_checkpoint(epoch, (epoch + 1) * self.steps_per_epoch, epoch_metrics)\n",
    "        \n",
    "        if self.local_rank == 0:\n",
    "            self.writer.close()\n",
    "            wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipeline = DataPipeline(data_dict, tokenizer)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    # Set is_distributed=True if you are doing distributed training\n",
    "    train_loader, val_loader, test_loader = pipeline.create_dataloaders(is_distributed=False)\n",
    "\n",
    "    # Initializing model\n",
    "    model = Transformer(**model_params)\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/input/tokenizer\")\n",
    "    # Initializaing Training Loop\n",
    "    config = TrainingConfig()\n",
    "    trainer = MTTrainer(config, model, tokenizer)\n",
    "    trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Transformer model configuration\"\"\"\n",
    "    vocab_size: int = 32000\n",
    "    d_model: int = 1024\n",
    "    num_heads: int = 16\n",
    "    num_layers: int = 24\n",
    "    d_ff: int = 4096\n",
    "    dropout: float = 0.1\n",
    "    max_sequence_length: int = 512\n",
    "    activation: str = \"gelu\"\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    pre_norm: bool = True  # Pre-norm vs Post-norm\n",
    "    attention_dropout: float = 0.1\n",
    "    position_embedding_type: str = \"rotary\"  # rotary, alibi, or learned\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training hyperparameters and settings\"\"\"\n",
    "    # Basic training params\n",
    "    num_epochs: int = 100\n",
    "    train_batch_size: int = 128\n",
    "    eval_batch_size: int = 64\n",
    "    gradient_accumulation_steps: int = 32\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer: str = \"adamw\"  # adamw, adafactor, lion\n",
    "    learning_rate: float = 1e-4\n",
    "    min_learning_rate: float = 1e-5\n",
    "    weight_decay: float = 0.01\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.998\n",
    "    adam_epsilon: float = 1e-8\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_scheduler: str = \"cosine_with_warmup\"  # linear, cosine, polynomial\n",
    "    warmup_steps: int = 2000\n",
    "    warmup_ratio: float = 0.01\n",
    "    \n",
    "    # Regularization\n",
    "    dropout: float = 0.1\n",
    "    label_smoothing: float = 0.1\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "    # Mixed precision training\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False  # bfloat16 support\n",
    "    fp16_opt_level: str = \"O2\"\n",
    "    \n",
    "    # Distributed training\n",
    "    distributed_strategy: str = \"ddp\"  # ddp, deepspeed, fsdp\n",
    "    ddp_find_unused_parameters: bool = False\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy: str = \"epoch\"  # steps, epoch\n",
    "    save_steps: int = 500\n",
    "    save_total_limit: int = 5\n",
    "    save_safetensors: bool = True\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    eval_steps: int = 500\n",
    "    eval_delay: int = 0\n",
    "    eval_timeout: int = 3600\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 5\n",
    "    early_stopping_threshold: float = 0.01\n",
    "    \n",
    "    # Logging\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 100\n",
    "    log_level: str = \"info\"\n",
    "    log_on_each_node: bool = True\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing: bool = True\n",
    "    torch_compile: bool = True  # PyTorch 2.0 compile\n",
    "    flash_attention: bool = True  # Use flash attention if available\n",
    "    \n",
    "    # Tokenizer settings\n",
    "    max_length: int = 512\n",
    "    padding: str = \"max_length\"\n",
    "    truncation: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data processing configuration\"\"\"\n",
    "    train_file: str = \"train.json\"\n",
    "    validation_file: str = \"validation.json\"\n",
    "    test_file: str = \"test.json\"\n",
    "    source_lang: str = \"en\"\n",
    "    target_lang: str = \"fr\"\n",
    "    \n",
    "    # Data processing\n",
    "    preprocessing_num_workers: int = 8\n",
    "    overwrite_cache: bool = False\n",
    "    max_train_samples: Optional[int] = None\n",
    "    max_eval_samples: Optional[int] = None\n",
    "    max_test_samples: Optional[int] = None\n",
    "    \n",
    "    # Augmentation and preprocessing\n",
    "    do_lowercase: bool = False\n",
    "    remove_punctuation: bool = False\n",
    "    character_coverage: float = 0.9995\n",
    "    temperature_sampling: bool = True\n",
    "    temperature: float = 5.0\n",
    "    \n",
    "    # Tokenizer settings\n",
    "    tokenizer_type: str = \"sentencepiece\"\n",
    "    vocab_size: int = 32000\n",
    "    special_tokens: Dict[str, str] = None\n",
    "\n",
    "@dataclass\n",
    "class MetricsConfig:\n",
    "    \"\"\"Evaluation metrics configuration\"\"\"\n",
    "    metrics: List[str] = None\n",
    "    bleu_type: str = \"sacrebleu\"\n",
    "    meteor: bool = True\n",
    "    rouge: bool = True\n",
    "    bleurt: bool = True\n",
    "    comet: bool = True\n",
    "    ter: bool = True\n",
    "    chrf: bool = True\n",
    "    \n",
    "    # Confidence estimation\n",
    "    use_confidence_estimation: bool = True\n",
    "    confidence_threshold: float = 0.8\n",
    "    \n",
    "    # Quality estimation\n",
    "    use_quality_estimation: bool = True\n",
    "    quality_estimation_model: str = \"OpenKiwi\"\n",
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    \"\"\"Inference configuration\"\"\"\n",
    "    beam_size: int = 5\n",
    "    length_penalty: float = 0.6\n",
    "    repetition_penalty: float = 1.0\n",
    "    no_repeat_ngram_size: int = 3\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    temperature: float = 1.0\n",
    "    diverse_beam_groups: int = 4\n",
    "    diverse_beam_strength: float = 0.5\n",
    "    \n",
    "    # Constrained decoding\n",
    "    force_words_ids: Optional[List[List[int]]] = None\n",
    "    suppress_tokens: Optional[List[int]] = None\n",
    "    \n",
    "    # Length control\n",
    "    min_length: int = 0\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Batch processing\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "\n",
    "def create_training_config(\n",
    "    model_name: str,\n",
    "    source_lang: str,\n",
    "    target_lang: str,\n",
    "    base_dir: str\n",
    ") -> Dict[str, Union[ModelConfig, TrainingConfig, DataConfig, MetricsConfig, InferenceConfig]]:\n",
    "    \"\"\"Create a complete training configuration\"\"\"\n",
    "    \n",
    "    # Initialize default configurations\n",
    "    model_config = ModelConfig()\n",
    "    training_config = TrainingConfig()\n",
    "    data_config = DataConfig(\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang\n",
    "    )\n",
    "    metrics_config = MetricsConfig(\n",
    "        metrics=[\n",
    "            \"bleu\",\n",
    "            \"meteor\",\n",
    "            \"ter\",\n",
    "            \"chrf\",\n",
    "            \"comet\",\n",
    "            \"bleurt\",\n",
    "            \"rouge\"\n",
    "        ]\n",
    "    )\n",
    "    inference_config = InferenceConfig()\n",
    "    \n",
    "    # Customization based on model size\n",
    "    if \"large\" in model_name.lower():\n",
    "        model_config.d_model = 1024\n",
    "        model_config.num_heads = 16\n",
    "        model_config.num_layers = 24\n",
    "        training_config.train_batch_size = 64\n",
    "    elif \"base\" in model_name.lower():\n",
    "        model_config.d_model = 768\n",
    "        model_config.num_heads = 12\n",
    "        model_config.num_layers = 12\n",
    "        training_config.train_batch_size = 128\n",
    "    \n",
    "    # Create paths\n",
    "    data_config.train_file = f\"{base_dir}/data/train.json\"\n",
    "    data_config.validation_file = f\"{base_dir}/data/validation.json\"\n",
    "    data_config.test_file = f\"{base_dir}/data/test.json\"\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_config,\n",
    "        \"training\": training_config,\n",
    "        \"data\": data_config,\n",
    "        \"metrics\": metrics_config,\n",
    "        \"inference\": inference_config\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "config = create_training_config(\n",
    "    model_name=\"mt-large\",\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"fr\",\n",
    "    base_dir=\"/path/to/project\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5906861,
     "sourceId": 9666947,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5986691,
     "sourceId": 9773422,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
