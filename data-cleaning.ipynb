{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-18T16:30:12.375894Z",
     "iopub.status.busy": "2024-10-18T16:30:12.375170Z",
     "iopub.status.idle": "2024-10-18T16:30:12.389301Z",
     "shell.execute_reply": "2024-10-18T16:30:12.387896Z",
     "shell.execute_reply.started": "2024-10-18T16:30:12.375832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/urdu-articles-raw/urdu_wikipedia_articles.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/kaggle/input/urdu-articles-raw/urdu_wikipedia_articles.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the first few entries (if it's a list or dict)\n",
    "# print(data[:5] if isinstance(data, list) else data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "content_count = 0\n",
    "\n",
    "# Assuming 'data' is a list of articles\n",
    "for entry in data:\n",
    "    if 'content' in entry:  # Check if 'content' key exists\n",
    "        content_count += 1  # Increment the count\n",
    "\n",
    "print(f\"Number of 'content' objects: {content_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T16:32:13.492469Z",
     "iopub.status.busy": "2024-10-18T16:32:13.492048Z",
     "iopub.status.idle": "2024-10-18T16:32:13.508951Z",
     "shell.execute_reply": "2024-10-18T16:32:13.507210Z",
     "shell.execute_reply.started": "2024-10-18T16:32:13.492430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Urdu normalization dictionary\n",
    "        self.urdu_normalization_dict = {\n",
    "            'أ': 'ا', 'إ': 'ا', 'ٱ': 'ا',\n",
    "            'ٻ': 'ب', 'ة': 'ت', 'ۃ': 'ت',\n",
    "            'ٿ': 'ث', 'ڃ': 'ج', 'ځ': 'ح',\n",
    "            'ڊ': 'د', 'ڒ': 'ر', 'ڛ': 'س',\n",
    "            'ڜ': 'ش', 'ڝ': 'ص', 'ڠ': 'ع',\n",
    "            'ڧ': 'غ', 'ڤ': 'ف', 'ڥ': 'ف', 'ڦ': 'ف',\n",
    "            'ڨ': 'ق', 'ڪ': 'ک', 'ڰ': 'گ', 'ڱ': 'گ',\n",
    "            'ڵ': 'ل', 'ݥ': 'م', 'ۆ': 'و', 'ۇ': 'و',\n",
    "            'ۈ': 'و', 'ۉ': 'و', 'ۊ': 'و', 'ۋ': 'و',\n",
    "            'ٔ': 'ء', 'ﺀ': 'ء',\n",
    "            '\\u064B': '', '\\u064C': '', '\\u064D': '', \n",
    "            '\\u064E': '', '\\u064F': '', '\\u0650': '', \n",
    "            '\\u0651': '', '\\u0652': '', '\\u0653': '', \n",
    "            '\\u0656': '', '\\u0657': '', '\\u0658': '', \n",
    "            '\\u0670': '',\n",
    "        }\n",
    "\n",
    "    def remove_non_urdu(self, text):\n",
    "        urdu_range = r'\\u0600-\\u06FF'\n",
    "        basic_punctuations = r'۔،؛؟!'\n",
    "        urdu_digits = r'۰-۹'\n",
    "        allowed_chars = urdu_range + urdu_digits + basic_punctuations\n",
    "        return re.sub(f'[^{allowed_chars}\\s]', '', text)\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    def normalize_urdu_specific(self, text):\n",
    "        pattern = re.compile('|'.join(map(re.escape, self.urdu_normalization_dict.keys())))\n",
    "        text = pattern.sub(lambda m: self.urdu_normalization_dict[m.group(0)], text)\n",
    "        text = re.sub(r'([اآءٰٓؤى])ا', r'\\1', text)\n",
    "        text = text.replace('ؤ', 'و')\n",
    "        text = re.sub(r'\\s*([۔،؛؟!])\\s*', r' \\1 ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_extra_spaces(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def remove_exact_duplicates(self, text):\n",
    "        sentences = re.split(r'(?<=[.!؟])\\s+', text.strip())\n",
    "        seen = set()\n",
    "        unique_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            normalized_sentence = self.normalize_urdu_specific(sentence)\n",
    "            if normalized_sentence and normalized_sentence not in seen:\n",
    "                seen.add(normalized_sentence)\n",
    "                unique_sentences.append(sentence)\n",
    "\n",
    "        return ' '.join(unique_sentences)\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.remove_non_urdu(text)\n",
    "        text = self.normalize_urdu_specific(text)\n",
    "        text = self.remove_extra_spaces(text)\n",
    "        text = self.remove_exact_duplicates(text)\n",
    "        return text\n",
    "\n",
    "    def clean_sentences(self, text):\n",
    "        cleaned_text = self.clean(text)\n",
    "        sentences = re.split(r'(?<=[۔!?])\\s*', cleaned_text)\n",
    "        return [s for s in sentences if s]  # Remove empty strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T16:32:15.501423Z",
     "iopub.status.busy": "2024-10-18T16:32:15.500969Z",
     "iopub.status.idle": "2024-10-18T16:32:15.510285Z",
     "shell.execute_reply": "2024-10-18T16:32:15.508895Z",
     "shell.execute_reply.started": "2024-10-18T16:32:15.501384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    \n",
    "    text = \"\"    \n",
    "    text = cleaner.clean(text)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = TextCleaner()\n",
    "    \n",
    "    # Extract and clean content from data\n",
    "    cleaned_articles = [cleaner.clean_sentences(entry['content']) for entry in data]\n",
    "\n",
    "    # Flatten the list of sentences for easier handling\n",
    "    all_sentences = [sentence for sentences in cleaned_articles for sentence in sentences]\n",
    "\n",
    "    # Optionally, print some cleaned sentences\n",
    "    for i, cleaned_sentence in enumerate(all_sentences[:5]):  # Print the first 5 cleaned sentences\n",
    "        print(f\"Cleaned Sentence {i+1}: {cleaned_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "count = len(all_sentences)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Write all sentences to a .txt file\n",
    "with open(\"cleaned_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in all_sentences:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:02:53.551577Z",
     "iopub.status.busy": "2024-10-18T13:02:53.550238Z",
     "iopub.status.idle": "2024-10-18T13:02:55.250639Z",
     "shell.execute_reply": "2024-10-18T13:02:55.249343Z",
     "shell.execute_reply.started": "2024-10-18T13:02:53.551525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:02:55.253485Z",
     "iopub.status.busy": "2024-10-18T13:02:55.252952Z",
     "iopub.status.idle": "2024-10-18T13:02:55.390898Z",
     "shell.execute_reply": "2024-10-18T13:02:55.389415Z",
     "shell.execute_reply.started": "2024-10-18T13:02:55.253435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e4)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:02:56.067291Z",
     "iopub.status.busy": "2024-10-18T13:02:56.066398Z",
     "iopub.status.idle": "2024-10-18T13:02:56.079731Z",
     "shell.execute_reply": "2024-10-18T13:02:56.078274Z",
     "shell.execute_reply.started": "2024-10-18T13:02:56.067239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:03:15.996475Z",
     "iopub.status.busy": "2024-10-18T13:03:15.995977Z",
     "iopub.status.idle": "2024-10-18T13:03:16.579111Z",
     "shell.execute_reply": "2024-10-18T13:03:16.577849Z",
     "shell.execute_reply.started": "2024-10-18T13:03:15.996428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:11:37.506955Z",
     "iopub.status.busy": "2024-10-18T13:11:37.506504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78247eff16f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/python-docx/\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T13:11:12.896579Z",
     "iopub.status.busy": "2024-10-18T13:11:12.896060Z",
     "iopub.status.idle": "2024-10-18T13:11:12.939636Z",
     "shell.execute_reply": "2024-10-18T13:11:12.938191Z",
     "shell.execute_reply.started": "2024-10-18T13:11:12.896533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Specify the path to the .txt file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m txt_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/clean-urdu-wiki/cleaned_sentences_wiki.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Specify the path to the .txt file\n",
    "txt_file_path = '/kaggle/input/clean-urdu-wiki/cleaned_sentences_wiki.txt'\n",
    "docx_file_path = 'cleaned_sentences_wiki.docx'\n",
    "\n",
    "# Create a new Document\n",
    "doc = Document()\n",
    "\n",
    "# Read and add paragraphs in a more efficient way\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        doc.add_paragraph(line.strip())\n",
    "\n",
    "# Save the .docx file\n",
    "doc.save(docx_file_path)\n",
    "\n",
    "print(f'Document created: {docx_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5899496,
     "sourceId": 9657064,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
