{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9666947,"sourceType":"datasetVersion","datasetId":5906861},{"sourceId":9771795,"sourceType":"datasetVersion","datasetId":5985437},{"sourceId":9771842,"sourceType":"datasetVersion","datasetId":5985478},{"sourceId":9773422,"sourceType":"datasetVersion","datasetId":5986691}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-12T05:00:40.595634Z","iopub.execute_input":"2025-01-12T05:00:40.596052Z","iopub.status.idle":"2025-01-12T05:00:41.738727Z","shell.execute_reply.started":"2025-01-12T05:00:40.595993Z","shell.execute_reply":"2025-01-12T05:00:41.737579Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/wiki-articles-2/cleaned_file.csv\n/kaggle/input/cc-matrix/ccmatrix_ur_en_train.csv\n/kaggle/input/opus-dataset/val.csv\n/kaggle/input/opus-dataset/train.csv\n/kaggle/input/opus-dataset/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ndef csv_to_txt(csv_files, output_txt):\n    \"\"\"\n    Convert multiple CSV files into a single TXT file.\n\n    Args:\n    csv_files (list of str): List of paths to the input CSV files.\n    output_txt (str): Path for the output TXT file.\n    \"\"\"\n    try:\n        with open(output_txt, 'w', encoding='utf-8') as txt_file:\n            for csv_file in csv_files:\n                # Read the CSV file\n                df = pd.read_csv(csv_file)\n\n                # Check if the DataFrame has exactly two columns\n                if df.shape[1] != 2:\n                    raise ValueError(f\"The CSV file {csv_file} must contain exactly two columns.\")\n\n                # Write each row to the TXT file\n                for _, row in df.iterrows():\n                    urdu_text = row.iloc[0]  # First column (Urdu) using iloc\n                    english_text = row.iloc[1]  # Second column (English) using iloc\n\n                    # Write the line to the TXT file\n                    # Adjust separator if needed (e.g., '\\t' for tab or ' ' for space)\n                    txt_file.write(f\"{urdu_text}\\t{english_text}\\n\")\n\n        print(f\"Successfully created {output_txt} with combined data from the CSV files.\")\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Example usage\ncsv_files = [\n    '/kaggle/input/cc-matrix/ccmatrix_ur_en_train.csv',   # Path to first CSV file\n    '/kaggle/input/opus-dataset/train.csv',  # Path to second CSV file\n    '/kaggle/input/opus-dataset/val.csv',\n    '/kaggle/input/wiki-articles-2/cleaned_file.csv',# Path to third CSV file\n    '/kaggle/input/opus-dataset/test.csv'\n]\noutput_txt_path = '/kaggle/working/combined_data.txt'  # Output TXT file path\n\ncsv_to_txt(csv_files, output_txt_path)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-12T05:01:05.519779Z","iopub.execute_input":"2025-01-12T05:01:05.520179Z","iopub.status.idle":"2025-01-12T05:09:16.489797Z","shell.execute_reply.started":"2025-01-12T05:01:05.520144Z","shell.execute_reply":"2025-01-12T05:09:16.487943Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Successfully created /kaggle/working/combined_data.txt with combined data from the CSV files.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"line_count = 0\nwith open(\"/kaggle/working/combined_data.txt\", 'r') as file:\n    for line in file:\n        line_count += 1\nprint(f\"Number of lines: {line_count}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-12T05:09:20.652843Z","iopub.execute_input":"2025-01-12T05:09:20.653324Z","iopub.status.idle":"2025-01-12T05:09:27.444718Z","shell.execute_reply.started":"2025-01-12T05:09:20.653277Z","shell.execute_reply":"2025-01-12T05:09:27.443548Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of lines: 7036078\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Tokenizer Training","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\nfrom tokenizers.models import Unigram\nfrom tokenizers.normalizers import Sequence, NFKD, StripAccents, Replace\nfrom tokenizers.pre_tokenizers import Metaspace\nfrom tokenizers.processors import TemplateProcessing\nfrom tokenizers.decoders import Metaspace as MetaspaceDecoder\nfrom transformers import PreTrainedTokenizerFast\nimport torch\n\nclass Normalizer:\n    def __init__(self):\n        self.normalizer = Sequence([\n            Replace(\"``\", '\"'),\n            Replace(\"''\", '\"'),\n            NFKD(),\n            StripAccents(),\n            Replace(r\" {2,}\", \" \")\n        ])\n\nclass PreTokenizer:\n    def __init__(self):\n        self.pre_tokenizer = Metaspace()\n\nclass PostProcessor:\n    def __init__(self, tokenizer):\n        # The tokenizer instance is passed to access token IDs after training.\n        self.tokenizer = tokenizer\n        self.post_processor = None  # Will be initialized in the setup method.\n\n    def setup(self):\n        # Set up the post-processor after the tokenizer is trained\n        self.post_processor = TemplateProcessing(\n            single=\"<s>:0 $A:0 </s>:1\",\n            pair=\"<s>:0 $A:0 </s>:1 $B:1 </s>:2\",\n            special_tokens=[\n                (\"<s>\", self.tokenizer.token_to_id(\"<s>\")),\n                (\"</s>\", self.tokenizer.token_to_id(\"</s>\")),\n            ]\n        )\n\nclass Decoder:\n    def __init__(self):\n        self.decoder = MetaspaceDecoder()\n\nclass TokenizerTrainer:\n    def __init__(self, vocab_size=50000, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]):\n        self.vocab_size = vocab_size\n        self.special_tokens = special_tokens\n\n    def train(self, tokenizer, corpus_file):\n        trainer = trainers.UnigramTrainer(\n            vocab_size=self.vocab_size,\n            special_tokens=self.special_tokens,\n            unk_token=\"<unk>\"\n        )\n        tokenizer.train_from_iterator(self.get_training_corpus(corpus_file), trainer=trainer)\n\n    @staticmethod\n    def get_training_corpus(corpus_file):\n        with open(corpus_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                yield line.strip()\n\nclass UnigramTokenizer:\n    def __init__(self, vocab_size=32000, corpus_file='vocab.txt'):\n        self.tokenizer = Tokenizer(Unigram())\n        self.wrapped_tokenizer = None\n\n        # Initialize components\n        self.normalizer = Normalizer()\n        self.pre_tokenizer = PreTokenizer()\n        self.trainer = TokenizerTrainer(vocab_size)\n        self.post_processor = PostProcessor(self.tokenizer)\n        self.decoder = Decoder()\n\n        self._initialize_tokenizer(corpus_file)\n\n    def _initialize_tokenizer(self, corpus_file):\n        # Set up the tokenizer components\n        self.tokenizer.normalizer = self.normalizer.normalizer\n        self.tokenizer.pre_tokenizer = self.pre_tokenizer.pre_tokenizer\n        self.trainer.train(self.tokenizer, corpus_file)\n\n        # Set up the post-processor after training the tokenizer\n        self.post_processor.setup()\n        self.tokenizer.post_processor = self.post_processor.post_processor\n\n        # Set up the decoder\n        self.tokenizer.decoder = self.decoder.decoder\n\n        # Wrap in PreTrainedTokenizerFast\n        self.wrapped_tokenizer = PreTrainedTokenizerFast(\n            tokenizer_object=self.tokenizer,\n            bos_token=\"<s>\",\n            eos_token=\"</s>\",\n            unk_token=\"<unk>\",\n            pad_token=\"<pad>\",\n            padding_side=\"right\",\n        )\n\n    def encode(self, text_samples, max_length=512, return_tensors='pt'):\n        \"\"\"\n        Encodes a batch of text samples.\n\n        Args:\n            text_samples (list or str): A list of sentences or a single sentence to be encoded.\n            max_length (int): Maximum length of the output sequences.\n            return_tensors (str): The type of tensor to return ('pt' for PyTorch, 'tf' for TensorFlow).\n\n        Returns:\n            Tensor: Encoded token IDs as a tensor of specified type.\n        \"\"\"\n        if isinstance(text_samples, str):\n            text_samples = [text_samples]  # Convert to a list if a single sentence is passed.\n\n        # Encode the batch of sentences\n        encoded = self.wrapped_tokenizer(\n            text_samples,\n            return_tensors=return_tensors,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n        )\n        \n        return encoded['input_ids']  # Return only the input IDs\n\n    def decode(self, encoded_batch, skip_special_tokens=True):\n        \"\"\"\n        Decodes a batch of encoded token IDs.\n\n        Args:\n            encoded_batch (Tensor): A tensor of encoded token IDs.\n            skip_special_tokens (bool): Whether to skip special tokens during decoding.\n\n        Returns:\n            list: A list of decoded sentences.\n        \"\"\"\n        # Decode each encoded sentence in the batch\n        decoded_batch = [self.wrapped_tokenizer.decode(encoded, skip_special_tokens=skip_special_tokens) for encoded in encoded_batch]\n        return decoded_batch\n\n    def save(self, directory):\n        self.wrapped_tokenizer.save_pretrained(directory)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-12T05:10:41.135454Z","iopub.execute_input":"2025-01-12T05:10:41.135858Z","iopub.status.idle":"2025-01-12T05:10:44.812626Z","shell.execute_reply.started":"2025-01-12T05:10:41.135820Z","shell.execute_reply":"2025-01-12T05:10:44.811506Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    tokenizer = UnigramTokenizer(corpus_file='/kaggle/working/combined_data.txt')\n\n    # Test batch of sentences\n    test_sentences = [\n        \"The sun sets in the west, painting the sky orange and pink.\",\n        \"She enjoys long walks on the beach during sunset.\",\n        \"I love to run.\"\n    ]\n\n    # Batch encode\n    encoded_batch = tokenizer.encode(test_sentences, max_length=100, return_tensors='tf')\n    print(f\"Encoded batch: {encoded_batch}\")\n\n    # Batch decode\n    decoded_batch = tokenizer.decode(encoded_batch, skip_special_tokens=False)\n    print(f\"Decoded batch: {decoded_batch}\")\n\n    # Save the tokenizer\n    tokenizer.save(\"./32k_tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-12T05:10:47.182289Z","iopub.execute_input":"2025-01-12T05:10:47.182857Z","iopub.status.idle":"2025-01-12T05:32:01.307317Z","shell.execute_reply.started":"2025-01-12T05:10:47.182797Z","shell.execute_reply":"2025-01-12T05:32:01.305934Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Encoded batch: [[    2   295  1275  6528    23     5  8075     9 19789     5  2053 26010\n     10     4   965 16970    11     3     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1]\n [    2  4120  2108    33   948 12525    71     5 14399  1305 19812    11\n      3     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1]\n [    2    58   486    16  2090    11     3     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1     1     1     1     1     1     1     1     1\n      1     1     1     1]]\nDecoded batch: ['<s> The sun sets in the west, painting the sky orange and pink.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<s> She enjoys long walks on the beach during sunset.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<s> I love to run.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\n# Load the tokenizer from the saved directory\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/input/tokenizer\")\n\n# Now you can use the loaded tokenizer to encode/decode text\ntest_sentences = [\n    \"The sun sets in the west, painting the sky orange and pink.\",\n    \"She enjoys long walks on the beach during sunset.\",\n    \"I love to run.\"\n]\n\n# Batch encode\nencoded_batch = tokenizer(test_sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\nprint(f\"Encoded batch: {encoded_batch['input_ids']}\")\n\n# Batch decode\ndecoded_batch = [tokenizer.decode(encoded, skip_special_tokens=False) for encoded in encoded_batch['input_ids']]\nprint(f\"Decoded batch: {decoded_batch}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/combined_data.txt\", 'r') as file:\n    sentences = file.readlines()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentences[:-10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}